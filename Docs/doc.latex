\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{nameref}
\usepackage{pifont}
\usepackage{hyperref}
\usepackage{listings}
\lstset{basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{red},
  keywordstyle=\color{blue}
}
\usepackage{fourier}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\title{MPI interversion compability with Singularity containers over Finis Terrae II using an Infiniband network}

\author{Manuel Simon Novoa}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Enter a short summary here. What topic do you want to investigate and why? What experiment did you perform? What were your main results and conclusion?
\end{abstract}

\section{Introduction 5 lines to max 1/2 page}
\label{sec:introduction}

VÃ­ctor says: Hola!

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce mattis mauris nec neque lacinia mattis. Mauris tincidunt sapien quis tortor malesuada ullamcorper. Curabitur quis mauris sed odio viverra facilisis vel a nibh. Proin et vehicula est. In dictum blandit odio sit amet laoreet. Integer blandit purus id dolor sollicitudin tempor. Praesent turpis erat, aliquet ac tellus nec, iaculis laoreet augue. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Ut malesuada magna velit, non varius dui lacinia ut. Nulla facilisi. Nulla vulputate malesuada nisl. Mauris mauris est, consequat id condimentum nec, viverra quis est. Curabitur fermentum, sem sed lacinia auctor, nisi tellus interdum tellus, a suscipit libero libero a nulla. Praesent eu felis volutpat, viverra metus sit amet, viverra purus. In volutpat porttitor velit vel efficitur.

In a auctor ligula. Proin finibus quam a nisl molestie malesuada. Donec a enim purus. Mauris vel nisl ultrices, fringilla diam nec, volutpat tortor. Fusce malesuada, nisi egestas lobortis malesuada, nibh odio faucibus nulla, quis porttitor ligula ante et mauris. Nullam sed nisl interdum, fermentum turpis ut, dapibus orci. Maecenas venenatis arcu mauris, ut ultrices sapien venenatis vitae. Fusce semper sed tellus in commodo. In ornare erat vitae tincidunt euismod. Vestibulum nisi ex, fringilla sit amet dictum vel, venenatis feugiat tellus. Pellentesque eu nibh faucibus, posuere magna ac, volutpat sem.

Mauris ut lobortis odio, sed hendrerit nulla. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas lacinia ac leo ac placerat. In fringilla dolor vel ipsum rutrum, at finibus libero posuere. Integer eu varius elit. Aliquam erat volutpat. Nulla sed risus libero. Aliquam elementum eleifend sapien eu tristique.  \cite{nano3}.

\section{Used technologies: what, why and how}
\label{sec:technologies}

TODO: writte a previous intro

\subsection{A brief introduction to Singularity}
Lorem ipsum

\subsection{Variables of the development environment}
How the containers will be done? Why? Tests...
\paragraph{Open MPI}
Selected versions... why $\rightarrow$ FT2
\paragraph{Intel MPI}
Same...

\subsection{Research of the development environment}
Bootstrap files and its creation. Short intro and raised possibilities.
\paragraph{Using the default version included in different Linux distributions}
Trying ubuntu, debian and red hat distros. Using policy command.
\paragraph{Installing different OpenMPI versions on Ubuntu 16.10 containers}
How is the software installed?
Bootstrap files and scripts for its creation (automation)

\subsection{Final containers creation proccess}
Example of a bootstrap definition file

\section{Study environment: availabilities and limitations}
\label{sec:environment}
something...
\subsection{What's the 'objetos de estudio'}

\begin{table}[h]
\centering
\caption{My caption}
\label{my-label}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container OpenMPI version} & \multicolumn{5}{c|}{FT2 OpenMPI version} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 &  &  &  &  &  \\ \hline
2.0.0 &  &  &  &  &  \\ \hline
2.0.1 &  &  &  &  &  \\ \hline
2.0.2 &  &  &  &  &  \\ \hline
2.1.1 &  &  &  &  & \\ \hline
\end{tabular}
\end{table}


\subsection{Available compilers}
gnu, intel...
\begin{figure}[h]
\centering\includegraphics[width=0.6\linewidth]{cubo-pruebas-sin-nombre-final}
\caption{Figure caption}
\end{figure}
\begin{figure}[h]
\centering\includegraphics[width=0.6\linewidth]{desglose-cubo}
\caption{Figure caption}
\end{figure}

\section{Approaches of the problem and development}
\label{sec:approach}
Reference to \nameref{sec:mixing} \newline
Reference to \nameref{sec:linking}

\section{Mixing MPI versions}
\label{sec:mixing}
\paragraph{Trying 'by default'}
\subparagraph{Test apps: ring, hellowo}
hellowo fue empleado en las de compilador intel

\subsection{mpi run}
\begin{table}[H]
\centering
\caption{mpirun mixing different compilers, 1 and 2 nodes}
\label{my-label}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container OpenMPI version} & \multicolumn{5}{c|}{FT2 OpenMPI version} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.0 & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.1 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.2 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} \\ \hline
2.1.1 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} \\ \hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{mpirun Intel 1 and 2 nodes, hellowo}
\label{my-label}
\begin{tabular}{|r|c|c|}
\hline
\multicolumn{1}{|c|}{Container Intel MPI version} & \multicolumn{2}{c|}{FT2 Intel MPI version} \\ \hline
 & 5.1 & 2017 \\ \hline
5.1 &  &  \\ \hline
2017 & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} \\ \hline
\end{tabular}
\end{table}

\subsection{srun (slurm)}
\begin{table}[H]
\centering
\caption{srun front FT2, different compilers, 1 node}
\label{my-label}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container OpenMPI version} & \multicolumn{5}{c|}{FT2 OpenMPI version} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.0 & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.1 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.2 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} \\ \hline
2.1.1 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} \\ \hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{srun front FT2, different compilers, 2 nodes}
\label{my-label}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container OpenMPI version} & \multicolumn{5}{c|}{FT2 OpenMPI version} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.0 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.1 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.2 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.1.1 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
\end{tabular}
\end{table}
When this test was done, false positives were obtained for every one-to-one match. In those cases, when the ring program was executed, it did an execution for every node, but none of them could "see" the others. Because of this reason, the ring call was executed as many instances as nodes were indicated, having each of them just one process per ring. So, parallel communication was not obtained.


\section{Binding libraries and its dependencies}
\label{sec:linking}
\subsection{mpi run}
\subsection{srun (slurm)}
\paragraph{ldd}
\subparagraph{Tests apps: ring, hellowo}

\begin{table}[H]
\centering
\caption{srun ring 2 nodes, mixing compilers}
\label{my-label}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container OpenMPI version} & \multicolumn{5}{c|}{FT2 OpenMPI version} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 & \color[HTML]{006600}{\cmark}  & \color[HTML]{FF4F00}{\danger} & \color[HTML]{FF4F00}{\danger} & \color[HTML]{FF4F00}{\danger} & \color[HTML]{FF4F00}{\danger} \\ \hline
2.0.0 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} \\ \hline
2.0.1 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} \\ \hline
2.0.2 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} \\ \hline
2.1.1 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} \\ \hline
\end{tabular}
\end{table}
As can be seen at the table above, once all the involved libraries and its dependencies were satisfied, the test program, ring, could be executed under any container-FT2 OpenMPI mix. However, if the 1.10.2 OpenMPI container is used with a superior OpenMPI version outside, it will show a warning prompt, making reference to  different sizes in symbols (e.g. Symbol 'ompi\_mpi\_comm\_world' has different size in shared object, consider re-linking). Due to this, future real world applications could not work properly or crash.

\subparagraph{Real world apps: Feel++, XH5For}
Once the test applications passed; real world applications, build inside already created containers, were proved. For this reason, the OpenMPI version contained could not be changed.
\begin{table}[H]
\centering
\caption{Feel++}
\label{my-label}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container OpenMPI version} & \multicolumn{5}{c|}{FT2 OpenMPI version} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} \\ \hline
\end{tabular}
\end{table}
As can be seen, in this test, a containerized 1.10.2 OpenMPI version worked over 1.10.2, 2.0.0, 2.0.1 and 2.0.2 outside OpenMPI versions. Nevertheless, 2.1.1 outside OpenMPI did not work. Most probably this is because the latest version employs an Intel compiler, and not a GNU compiler as the other ones. So, for the very first time, a compiler dependency was discover. In order to get reduce noise as much as possible, from this moment, they are not taken into account OpenMPI versions that have not been compiled with GNU compilers.

\begin{table}[H]
\centering
\caption{XH5For}
\label{my-label}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container OpenMPI version} & \multicolumn{5}{c|}{FT2 OpenMPI version} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} \\ \hline
2.1.1 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} \\ \hline
\end{tabular}
\end{table}

\section{Benchmarks}
One of the biggest containers' advantages is its theoretical efficient and lightweight operating mode. When using the same kernel as the host machine, and avoiding replication as much as possible, makes of containers virtualization a great solution for HPC programs execution. \newline
Nevertheless, until this lightness is verified in our work environment, we will not stop talking about a completely theoretical framework. Because of this, some benchmarks will be done, to verify this in a practical way.
\subsection{Network benchmarks: OSU microbenchmarks}
The selected bechmarks for this practical approach are the \textit{"OSU microbenchmarks"}\cite{osu-microbenchmarks}, belonging to \textit{"The Ohio State University"}, what allow us to measure some interesting parameters, such as latency or bandwidth. Once the tests were done, we will check if the obtained results belong to a reasonable Infiniband network parameters. \newline
So, the objective of the following benchmark tests is no other than proof that containers development will allow a comparable response time, versus a native execution. \newline
Once the \textit{"OSU microbenchmarks"} are installed, we can find a battery of test in the directory \verb|/usr/bin/libexec/osu-micro-benchmarks| (default installation directory). This battery of benchmarks is composed by the following:
\begin{lstlisting}[language=bash,caption={OSU microbenchmarks}]
.
`-- osu-micro-benchmarks
    `-- mpi
        |-- collective
        |   |-- osu_allgather
        |   |-- osu_allgatherv
        |   |-- osu_allreduce
        |   |-- osu_alltoall
        |   |-- osu_alltoallv
        |   |-- osu_barrier
        |   |-- osu_bcast
        |   |-- osu_gather
        |   |-- osu_gatherv
        |   |-- osu_iallgather
        |   |-- osu_iallgatherv
        |   |-- osu_ialltoall
        |   |-- osu_ialltoallv
        |   |-- osu_ialltoallw
        |   |-- osu_ibarrier
        |   |-- osu_ibcast
        |   |-- osu_igather
        |   |-- osu_igatherv
        |   |-- osu_iscatter
        |   |-- osu_iscatterv
        |   |-- osu_reduce
        |   |-- osu_reduce_scatter
        |   |-- osu_scatter
        |   `-- osu_scatterv
        |-- one-sided
        |   |-- osu_acc_latency
        |   |-- osu_cas_latency
        |   |-- osu_fop_latency
        |   |-- osu_get_acc_latency
        |   |-- osu_get_bw
        |   |-- osu_get_latency
        |   |-- osu_put_bibw
        |   |-- osu_put_bw
        |   `-- osu_put_latency
        |-- pt2pt
        |   |-- osu_bibw
        |   |-- osu_bw
        |   |-- osu_latency
        |   |-- osu_latency_mt
        |   |-- osu_mbw_mr
        |   `-- osu_multi_lat
        `-- startup
            |-- osu_hello
            `-- osu_init
\end{lstlisting}
The working of these different benchmarks can be consulted on the \verb|README| file which is downloaded along with the benchmarks. If we do not want to download the OSU microbenchmarks package to read this file, it can be also consulted online\cite{osu-microbenchmarks-readme}. \newline
As can be seen, a lot of different benchmarks are available on this package. We will focus our attention on the point-to-point (pt2pt) tests, what will be good approximation to the most of the real world applications' behavior.
\subsection{RAM benchmarks: STREAM}
The STREAM benchmark is a simple synthetic benchmark program that measures sustainable memory bandwidth (in MB/s) and the corresponding computation rate for simple vector kernels. The STREAM benchmark uses data sets much larger than the available cache on a system, which avoids large amounts of time devoted waiting for cache misses to be satisfied. Because of this reason, STREAM documentation indicates that the value of the \verb|STREAM_ARRAY_SIZE| must be at least four times larger than the combined size of all last level caches used in the run. But this is not the only limitation to define the value of \verb|STREAM_ARRAY_SIZE|: Its size should be large enough so that the "timing calibration" output by the program is at least 20 clock-ticks. Example: most versions of Windows have a 10 millisecond timer granularity. 20 "ticks" at 10 ms/tic is 200 milliseconds. If the chip is capable of 10 GB/s, it moves 2 GB in 200 msec. This means the each array must be at least 1 GB, or 128M elements. \newline
Turning to our particular case, Finis Terrae II nodes have 24 cores, with the following memory cache structure:
\begin{itemize}
\item L1d cache: 32K
\item L1i cache: 32K
\item L2 cache: 256K
\item L3 cache: 30720K
\end{itemize}
So, attending to the previous limitations to define the value of the \verb|STREAM_ARRAY_SIZE|, it is important to consider the values of the last level cache, as well as the number of available cores. Doing some arithmetic, we can observe that the sum total of the last level cache is 720MB (30720K = 30MB per core). Then, a 720MB four times size will be needed (2880MB should be a correct array size). I will define the value of the \verb|STREAM_ARRAY_SIZE| as 380000000, which should be a value larger enough for our porposes. \newline
The other limitation factor, the "timing calibration" output by the program, can not be considered until the program is executed, so it will be ignored at the moment. \newline
Continuing with the values under the benchmark will be done, it is important to consider that STREAM runs each kernel \verb|NTIMES| times and reports the best result for any iteration after the first. The selected value for this variable will be the default one: 10 times. \newline
Finally, keep in mind that we must compile the code with optimization and enabling the OpenMP options to be able to use multiprocessing.


\section{Discussion 1/2-1 page}
Ut eget est sem. Duis suscipit turpis sed orci mattis, sed tempus sapien rhoncus. Aliquam at ex nulla. Suspendisse et lorem ornare, tincidunt eros et, dictum metus. Etiam auctor elementum enim. Fusce in convallis ex, at sagittis elit. Suspendisse auctor gravida molestie. In mi tellus, tristique quis mi sed, blandit lobortis libero. 


\begin{thebibliography}{9}
\bibitem{nano3}
  K. Grove-Rasmussen og Jesper NygÃ¥rd,
  \emph{KvantefÃ¦nomener i Nanosystemer}.
  Niels Bohr Institute \& Nano-Science Center, KÃ¸benhavns Universitet
\bibitem{osu-microbenchmarks}
  OSU Micro Benchmarks [Internet] - MVAPICH: MPI over InfiniBand, Omni-Path, Ethernet/iWARP, and RoCE [Quoted July 24, 2017]. 
Recovered from: \url{<http://mvapich.cse.ohio-state.edu/userguide/virt/#_osu_micro_benchmarks>}
\bibitem{osu-microbenchmarks-readme}
  OSU Micro Benchmarks README file [Internet] - MVAPICH: MPI over InfiniBand, Omni-Path, Ethernet/iWARP, and RoCE [Quoted July 24, 2017]. 
Recovered from: \url{<http://mvapich.cse.ohio-state.edu/static/media/mvapich/README-OMB.txt>}

\end{thebibliography}
\end{document}
