\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{nameref}
\usepackage{pifont}
\usepackage{hyperref}
\usepackage{indentfirst}
\usepackage{lipsum}
\usepackage{listings}
\lstset{basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{red},
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
  keywordstyle=\color{blue}
}
\usepackage{fourier}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand\hr{\par\vspace{-.5\ht\strutbox}\noindent\hrulefill\par}

\title{MPI interversion compability with Singularity containers over Finis Terrae II using an InfiniBand network}

\author{Manuel Simon Novoa}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Enter a short summary here. What topic do you want to investigate and why? What experiment did you perform? What were your main results and conclusion?
\end{abstract}

\section{Introduction 5 lines to max 1/2 page}
\label{sec:introduction}

\section{Architecture description}
\label{sec:architecture}
The tests and executions named in the following document were launched it all under the hardware described in this section. As this research belong to the \textit{Supercomputing Center of Galicia (CESGA)}, the selected machine to perform the development was the \textit{Finis Terrae II}.

\textit{Finis Terrae} is a scientific computing infrastructure managed by \textit{CESGA} and integrated in RES (the Spanish Supercomputing Network), a Spanish Unique Scientific and Technical Installation (ICTS). It is a linux based heterogeneous cluster, with an Infiniband FDR low latency network interconnecting 317 computing nodes based on Intel Xeon Hasswell processors. Together, these nodes are able to provide a computing power of 328 TFLOPS, 44,8 TB of RAM and 1,5 PB of disk capacity. The system includes a high performance parallel storage system able to achieve a speed of more than 20 GB/s.

\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{infraFTII}
      \caption{Finis Terrae II Infrastructure}
       \label{Finis Terrae II Infrastructure}
\end{figure}

\section{Research motivation}
\label{sec:motivation}
Traditional deployment of software performed in HPCs present some issues which make it very inflexible. Different approaches, like could be the containerization technology, can provide an interesting alternative that is more flexible with very little impact on performance.

Traditional software deployment and integration consists in performing all the needed steps, like download, configure, build, test and install, to have the software project natively in a production infrastructure. The main goal of this task is to provide the software with good performance and ready to use for end users.

Scientific software is extremely complex from an architectural point of view. It is usually composed of numerous mathematical concepts and features implemented along several software components in order to provide high level abstraction layers. These layers can be implemented in the software project itself or integrated via third party libraries or software components. The whole environment of each scientific software is usually composed of a complex dependency matrix and, at least, one programming language and compiler.

Isolation and integration of all dependency matrices at HPC clusters are traditionally managed by environment modules. Environment Modules provide a way to dynamically change the user environment through module files. Lmod, a Lua based module system, is being used at FinisTerrae II.

The key advantage of environment modules is that it allows to use multiple versions of a program or package from the same account by just loading the proper module file. In general, module files are created on per application per version basis. They can be dynamically loaded, unloaded, or switched. Along with the capability of using multiple versions of the same software it also can be used to implement site policies regarding the access and use of applications.

Module files allow managing the loading and unloading of environments to run a particular application, but to manage complex work-flows with environment modules can be sometimes unaffordable, and requires the re-installation of some tools with compatible dependencies. These issues are difficult to manage from the user and the administrator point of view.

Finally, the hardware and software ecosystem of an HPC production infrastructure is different than a development ecosystem, and usually a lot of unexpected issues appear while integrating and deploying the complex environment of mathematical frameworks. To integrate the whole environment of each software project in a production infrastructure is usually hard and time consuming.

Also,as this software is evolving very fast and using the latest technologies and features of the compilers, new versions are provided very frequently. This puts a lot of pressure on the software support team of the infrastructures. Even though there are some tools that could help in this process, like \textit{EasyBuild}\cite{easybuild}, an application to manage software on High Performance Computing (HPC) systems in an efficient way.

So, what is intended to do is to find a simple and efficient way that allow to deploy and integrate that complex software with its dependencies. The goal is to reduce at maximum at possible the previously named time consuming. Most importantly, because end users can come up with endless combinations of software and dependencies, it is to achieve maximum portability, but without losing the computational capacity of the system.

\section{Technology selection}
\label{sec:technology-selection}
In accordance with the requirements mentioned in the previous section, a study of the state of the art was made to select the solution that best fits the presented problem.

It is very important to always keep in mind these premises, since they are the ones that will limit the selection of the solution:
\begin{itemize}
\item The operating system of the cluster can not be modified, since there are other jobs that are currently running in the same without any problem, and it is not possible to hinder its execution.
\item We must make use of the available hardware (shown in section \nameref{sec:architecture}) as efficiently as possible; that is, we can not limit the resources (CPU, memory, network, etc.)
\end{itemize}

Thus, some common virtualization solutions are:

\subsection{Emulation}
An emulator typically functions as an interpreter who reads the instructions from the virtual operating system (guest) and translates them into instructions that can run smoothly on the host operating system. In this way the emulator provides the virtual operating system with a simulated execution environment. The emulator itself runs like any other application on the host operating system. The emulated hardware may or may not match the actual machine on which the emulator is running. Emulators recreate a complete environment, including all CPU instructions and I/O devices using software, which is why performance is deeply affected.

Therefore, because of that problem with performance, the second premise can not be accomplished. Emulation solution does not seem to be the best.

\subsection{Virtualization of the Operating System - Containerization} 
In this type of virtualization runs only a kernel, the host, which also attends calls from virtual systems. Virtualization software (usually called a container) uses a set of libraries that translate virtual operating system calls into the appropriate format to be executed on the hardware using the host operating system. The environment provided by the container may or may not match the actual machine on which it is run. In case it does not match, the translator will be quite complex. Each container runs in its own environment, although they all share the same resources. It is possible to reallocate these resources to implement quality of service. In addition, operating system virtualization has the advantage that overhead is minimal.

Therefore, this is a solution that allows flexibility in addition to a minimum overhead. 

Although the containerization techniques is a buzzword nowadays especially in the Datacenter and Cloud industry, the idea is quite old. Container or "chroot" (change root) was a Linux technology to isolate single processes from each other without the need of emulating different hardware for them. Containers are lightweight operating systems within the Host Operating system that runs them. It uses native instructions on the core CPU, without the requirement of any VMM (Virtual Machine Manager). The only limitation is that we have to use the Host operating systems kernel to use the existing hardware components, unlike with virtualization, where we could use different operating systems without any restriction at the cost of the performance overhead. This is the key feature for the project. We could use different software, libraries even different Linux distribution without reinstalling the system. This makes HPC systems more flexible and easy to use for scientists and developers.

Container technology has become very popular as it makes application deployment very easy and efficient. As people move from virtualization to container technology, many enterprises have adopted software container for cloud application deployment. There is a stiff competition to push different technologies in the market. Although Docker is the most popular, to choose the right technology, depending on the purpose, it is important to understand what each of them stands for and does.

Several containerization technologies (like LXC, Docker, Udocker and Singularity) have been tested in the context of this project, but finally, Docker was rejected because of its kernel requirements and security. For example FinisTerrae-II does not have the kernel required by Docker. Udocker and Singularity were developed specifically to be used in HPC environments, as we will describe below. Both of them are Docker-compatible, and help to empower end-users of HPC systems providing a contained location where to manage their installations and custom software. They are also a great solution for developers, one of the biggest benefits for them is to deliver their software in a controlled environment and ready-to-use.

In one hand, Udocker is a basic user tool to execute simple Docker containers in user space without requiring root privileges, which enables basic download and sequential execution of docker containers by non-privileged users in Linux systems. It can be used to access and execute the content of docker containers in Linux batch systems and interactive clusters that are managed by other entities such as grid infrastructures or externally managed batch or interactive systems.

Although the Udocker development team is working to integrate it with message passing interface libraries (MPI), unfortunately, it is not yet supported.

On the other hand, Singularity was designed focusing on HPC and allows to leverage the resources of whatever host in which the software is running. This includes HPC interconnects, resource managers, file systems, GPUs and/or accelerators, etc.

Singularity was also designed around the notion of extreme mobility of computing and reproducible science. Singularity is also used to perform HPC in the cloud on AWS, Google Cloud, Azure and other cloud providers. This makes it possible to develop a research work-flow on a laboratory or a laboratory server, then bundle it to run on a departmental cluster, on a leadership class supercomputer, or in the cloud.

The simple usage of Singularity allows users to manage the creation of new containers and also to run parallel applications easily. Figure \nameref{Singularity work-flow at Finis Terrae II} shows the work-flow for creating and running containers using Singularity at Finis Terrae II.

\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{singularity-workflow}
      \caption{Singularity work-flow at Finis Terrae II}
       \label{Singularity work-flow at Finis Terrae II}
\end{figure}

As we can see in the previous figure, users can create or pull images from public registries (like Docker Hub\cite{dockerhub} or Singularity Hub\cite{singularityhub}), and also import images from tar pipes. Once the image is created, singularity allows to execute the container in interactive mode, and test or run any contained application using batch systems. All the work-flow can be managed by a normal user at Finis Terrae II, except the bootstrap process that needs to be called by a superuser. We can use a virtual machine with superuser privileges to modify or adapt and image to the infrastructure using the bootstrap Singularity command.

This work-flow allow an automatized integration and deployment, deriving in a very flexible and portable solution.

\section{Used technologies: what, why and how}
\label{sec:technologies}

TODO: writte a previous intro

\subsection{A brief introduction to Singularity}
Lorem ipsum

\subsection{Variables of the development environment}
How the containers will be done? Why? Tests...
\paragraph{Open MPI}
Selected versions... why $\rightarrow$ FT2
\paragraph{Intel MPI}
Same...

\subsection{Research of the development environment}
Bootstrap files and its creation. Short intro and raised possibilities.
\paragraph{Using the default version included in different Linux distributions}
Trying ubuntu, debian and red hat distros. Using policy command.
\paragraph{Installing different OpenMPI versions on Ubuntu 16.10 containers}
How is the software installed?
Bootstrap files and scripts for its creation (automation)

\subsection{Final containers creation proccess}
Example of a bootstrap definition file

\section{Study environment: availabilities and limitations}
\label{sec:environment}
something...
\subsection{What's the 'objetos de estudio'}

\begin{table}[h]
\centering
\caption{My caption}
\label{my-label}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container OpenMPI version} & \multicolumn{5}{c|}{FT2 OpenMPI version} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 &  &  &  &  &  \\ \hline
2.0.0 &  &  &  &  &  \\ \hline
2.0.1 &  &  &  &  &  \\ \hline
2.0.2 &  &  &  &  &  \\ \hline
2.1.1 &  &  &  &  & \\ \hline
\end{tabular}
\end{table}


\subsection{Available compilers}
gnu, intel...
\begin{figure}[H]
\centering\includegraphics[width=0.6\linewidth]{cubo-pruebas-sin-nombre-final}
\caption{Figure caption}
\end{figure}
\begin{figure}[H]
\centering\includegraphics[width=0.6\linewidth]{desglose-cubo}
\caption{Figure caption}
\end{figure}

\section{Approaches of the problem and development}
\label{sec:approach}
Reference to \nameref{sec:mixing} \newline
Reference to \nameref{sec:linking}

\section{Mixing MPI versions}
\label{sec:mixing}
\paragraph{Trying 'by default'}
\subparagraph{Test apps: ring, hellowo}
hellowo fue empleado en las de compilador intel

\subsection{mpi run}
\begin{table}[H]
\centering
\caption{mpirun mixing different compilers, 1 and 2 nodes}
\label{my-label}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container OpenMPI version} & \multicolumn{5}{c|}{FT2 OpenMPI version} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.0 & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.1 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.2 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} \\ \hline
2.1.1 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} \\ \hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{mpirun Intel 1 and 2 nodes, hellowo}
\label{my-label}
\begin{tabular}{|r|c|c|}
\hline
\multicolumn{1}{|c|}{Container Intel MPI version} & \multicolumn{2}{c|}{FT2 Intel MPI version} \\ \hline
 & 5.1 & 2017 \\ \hline
5.1 &  &  \\ \hline
2017 & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} \\ \hline
\end{tabular}
\end{table}

\subsection{srun (slurm)}
\begin{table}[H]
\centering
\caption{srun front FT2, different compilers, 1 node}
\label{my-label}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container OpenMPI version} & \multicolumn{5}{c|}{FT2 OpenMPI version} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.0 & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.1 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.2 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} \\ \hline
2.1.1 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} \\ \hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{srun front FT2, different compilers, 2 nodes}
\label{my-label}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container OpenMPI version} & \multicolumn{5}{c|}{FT2 OpenMPI version} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.0 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.1 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.2 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.1.1 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
\end{tabular}
\end{table}
When this test was done, false positives were obtained for every one-to-one match. In those cases, when the ring program was executed, it did an execution for every node, but none of them could "see" the others. Because of this reason, the ring call was executed as many instances as nodes were indicated, having each of them just one process per ring. So, parallel communication was not obtained.


\section{Binding libraries and its dependencies}
\label{sec:linking}
\subsection{mpi run}
\subsection{srun (slurm)}
\paragraph{ldd}
\subparagraph{Tests apps: ring, hellowo}

\begin{table}[H]
\centering
\caption{srun ring 2 nodes, mixing compilers}
\label{my-label}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container OpenMPI version} & \multicolumn{5}{c|}{FT2 OpenMPI version} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 & \color[HTML]{006600}{\cmark}  & \color[HTML]{FF4F00}{\danger} & \color[HTML]{FF4F00}{\danger} & \color[HTML]{FF4F00}{\danger} & \color[HTML]{FF4F00}{\danger} \\ \hline
2.0.0 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} \\ \hline
2.0.1 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} \\ \hline
2.0.2 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} \\ \hline
2.1.1 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} \\ \hline
\end{tabular}
\end{table}
As can be seen at the table above, once all the involved libraries and its dependencies were satisfied, the test program, ring, could be executed under any container-FT2 OpenMPI mix. However, if the 1.10.2 OpenMPI container is used with a superior OpenMPI version outside, it will show a warning prompt, making reference to  different sizes in symbols (e.g. Symbol 'ompi\_mpi\_comm\_world' has different size in shared object, consider re-linking). Due to this, future real world applications could not work properly or crash.

\subparagraph{Real world apps: Feel++, XH5For}
Once the test applications passed; real world applications, build inside already created containers, were proved. For this reason, the OpenMPI version contained could not be changed.
\begin{table}[H]
\centering
\caption{Feel++}
\label{my-label}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container OpenMPI version} & \multicolumn{5}{c|}{FT2 OpenMPI version} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} \\ \hline
\end{tabular}
\end{table}
As can be seen, in this test, a containerized 1.10.2 OpenMPI version worked over 1.10.2, 2.0.0, 2.0.1 and 2.0.2 outside OpenMPI versions. Nevertheless, 2.1.1 outside OpenMPI did not work. Most probably this is because the latest version employs an Intel compiler, and not a GNU compiler as the other ones. So, for the very first time, a compiler dependency was discover. In order to get reduce noise as much as possible, from this moment, they are not taken into account OpenMPI versions that have not been compiled with GNU compilers.

\begin{table}[H]
\centering
\caption{XH5For}
\label{my-label}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container OpenMPI version} & \multicolumn{5}{c|}{FT2 OpenMPI version} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} \\ \hline
2.1.1 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} \\ \hline
\end{tabular}
\end{table}

\section{Benchmarks}


One of the biggest containers' advantages is its theoretical efficient and lightweight operating mode. When using the same kernel as the host machine, and avoiding replication as much as possible, makes of containers virtualization a great solution for HPC programs execution.


Nevertheless, until this lightness is verified in our work environment, we will not stop talking about a completely theoretical framework. Because of this, some benchmarks will be done, to verify this in a practical way.


So, the objective of the following benchmark tests is no other than proof that containers development will allow a great response time.
\subsection{InfiniBand network benchmarks: OSU microbenchmarks}
Since the goal of this document is the description of how to use Singularity containers employing different versions of MPI, one of the most interesting benchmarks that can be done is the use of the network. We are not talking about a network that connects with the outside, but an internode network that allows the communication of the different processors for a correct parallel operation. For this purpose, FinisTerrae II, as well as many others clusters, uses an InfiniBand network. InfiniBand is a computer-networking communications standard used in high-performance computing that features very high throughput and very low latency. It is used for data interconnect both among and within computers.


So, now the Singularity containers are using MPI, we will check if they are making use of the InfiniBand network for that. 


The selected bechmarks for this practical approach are the \textit{OSU microbenchmarks}\cite{osu-microbenchmarks}, belonging to \textit{The Ohio State University}, what will allow us to measure some interesting parameters, such as latency or bandwidth. Once the tests were done, we will check if the obtained results belong to a reasonable InfiniBand network parameters. The objective of this test is not to obtain the best possible results in relation to the network, but only to see if the containers are actually making use of the InfiniBand network, so no real optimizations were done with the network.


When the \textit{OSU microbenchmark} pack is downloaded in our container, it can be compilled and installed easily thanks to the included makefile.
\begin{lstlisting}[language=bash,caption={Installing OSU microbenchmarks}]
sudo singularity exec -w CONTAINER_NAME.img ./configure CC=/path/to/special/mpicc --prefix=<path-to-install> && make && make install
\end{lstlisting}


Once the \textit{OSU microbenchmarks} are installed, we can find a battery of test in the directory \verb|/usr/bin/libexec/osu-micro-benchmarks| (default installation directory). This battery of benchmarks is composed by the following:
\begin{lstlisting}[language=bash,caption={OSU microbenchmarks}]
.
`-- osu-micro-benchmarks
    `-- mpi
        |-- collective
        |   |-- osu_allgather
        |   |-- osu_allgatherv
        |   |-- osu_allreduce
        |   |-- osu_alltoall
        |   |-- osu_alltoallv
        |   |-- osu_barrier
        |   |-- osu_bcast
        |   |-- osu_gather
        |   |-- osu_gatherv
        |   |-- osu_iallgather
        |   |-- osu_iallgatherv
        |   |-- osu_ialltoall
        |   |-- osu_ialltoallv
        |   |-- osu_ialltoallw
        |   |-- osu_ibarrier
        |   |-- osu_ibcast
        |   |-- osu_igather
        |   |-- osu_igatherv
        |   |-- osu_iscatter
        |   |-- osu_iscatterv
        |   |-- osu_reduce
        |   |-- osu_reduce_scatter
        |   |-- osu_scatter
        |   `-- osu_scatterv
        |-- one-sided
        |   |-- osu_acc_latency
        |   |-- osu_cas_latency
        |   |-- osu_fop_latency
        |   |-- osu_get_acc_latency
        |   |-- osu_get_bw
        |   |-- osu_get_latency
        |   |-- osu_put_bibw
        |   |-- osu_put_bw
        |   `-- osu_put_latency
        |-- pt2pt
        |   |-- osu_bibw
        |   |-- osu_bw
        |   |-- osu_latency
        |   |-- osu_latency_mt
        |   |-- osu_mbw_mr
        |   `-- osu_multi_lat
        `-- startup
            |-- osu_hello
            `-- osu_init
\end{lstlisting}


The working of these different benchmarks can be consulted on the \verb|README| file which is downloaded along with the benchmarks. If we do not want to download the \textit{OSU microbenchmarks} package to read this file, it can be also consulted online\cite{osu-microbenchmarks-readme}.


As can be seen, a lot of different benchmarks are available on this package. We will focus our attention on the point-to-point (pt2pt) tests, what will allow us to measure and to evaluate the network's characteristics from the own containers.

\paragraph{Latency Tests}
\subparagraph{osu\_latency - Latency Test}
The latency tests are carried out in a ping-pong fashion. The sender sends a message with a certain data size to the receiver and waits for a reply from the receiver. The receiver receives the message from the sender and sends back a reply with the same data size. Many iterations of this ping-pong test are carried out and average one-way latency numbers are obtained. Blocking version of MPI functions (MPI\_Send and MPI\_Recv) are used in the tests.
\subparagraph{osu\_multi\_lat - Multi-pair Latency Test}
This test is very similar to the latency test. However, at the same instant multiple pairs are performing the same test simultaneously. In order to perform the test across just two nodes the hostnames must be specified in block fashion. 

\paragraph{Bandwidth Tests}
\subparagraph{osu\_bw - Bandwidth Test}
The bandwidth tests were carried out by having the sender sending out a fixed number (equal to the window size) of back-to-back messages to the receiver and then waiting for a reply from the receiver. The receiver sends the reply only after receiving all these messages. This process is repeated for several iterations and the bandwidth is calculated based on the elapsed time (from the time sender sends the first message until the time it receives the reply back from the receiver) and the number of bytes sent by the sender. The objective of this bandwidth test is to determine the maximum sustained date rate that can be achieved at the network level. Thus, non-blocking version of MPI functions (MPI\_Isend and MPI\_Irecv) were used in the test. 

\subparagraph{osu\_bibw - Bidirectional Bandwidth Test}
The bidirectional bandwidth test is similar to the bandwidth test, except that both the nodes involved send out a fixed number of back-to-back messages and wait for the reply. This test measures the maximum sustainable aggregate bandwidth by two nodes.

\subparagraph{osu\_mbw\_mr - Multiple Bandwidth / Message Rate Test}
The multi-pair bandwidth and message rate test evaluates the aggregate uni-directional bandwidth and message rate between multiple pairs of processes. Each of the sending processes sends a fixed number of messages (the window size) back-to-back to the paired receiving process before waiting for a reply from the receiver. This process is repeated for several iterations. The objective of this benchmark is to determine the achieved bandwidth and message rate from one node to another node with a configurable number of processes running on each node. 


\hr
Having understood the purpose and behavior of the tests to be performed, we proceed to explain how they will be executed. We will make use of a little battery of five containers with Ubuntu 16.04.2 LTS (Ubuntu Xenial). Each container will have installed inside different Open MPI versions: 1.10.2, 2.0.0. 2.0.1, 2.0.2 and 2.1.1. Then, the \textit{OSU microbenchmarks} will be executed under the Finis Terrae II employing these containers, and mixing them with different Open MPI versions on it (outside the containers). The selected outside Open MPI versions are: 1.10.2, 2.0.0 and 2.0.1. So, a total of fifteen executions per test will be done, in order to obtain more stable values.


To ensure the use of the InfiniBand network, the obtained values will be compared with some scientific papers that tell how they used the same tests to measure their own InfiniBand network. These texts belong to research centers or similar, so they will be considered completely reliable sources. Some of them compare the InfiniBand network values versus the Ethernet network values obtained under the same tests. So, if the results obtained under the Finis Terrae II hover around similar values, the InfiniBand network use will be assured.


Once the different benchmarks were executed, it is easily observable that the obtained results under the same Open MPI version out of the container are very stable for the five Open MPI version combinations within the container. However, this does not happen when we talk about total Open MPI combinations outside the containers (although in some cases it remains completely stable). Observed this behavior, what has been done is to perform the average of the values belonging to the same version of Open MPI outside the container, to perform a comparison between the three versions of Open MPI outside the container.

\paragraph{Latency Tests}
\subparagraph{osu\_latency - Latency Test} As can be shown in figure \nameref{OSU Latency Test}, all tests showed very stable results regardless of the version of Open MPI used. The latency values shown are very small, less than 6 microseconds with window sizes up to 8192 bytes.
\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{osu_latency}
      \caption{OSU Latency Test}
       \label{OSU Latency Test}
\end{figure}

\subparagraph{osu\_multi\_lat - Multi-pair Latency Test} For the execution of this test, four cores were employed, in four different nodes. Results can be observed in figure \nameref{OSU Multi-pair Latency Test}. It seems that by extending the complexity of the problem, making multiple pairs to perform the same test simultaneously, at the same instant, not suppose a problem for the latency. The obtained results show one more time an extraordinary stability and a very low latency.
\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{osu_multi_lat}
      \caption{OSU Multi-pair Latency Test}
       \label{OSU Multi-pair Latency Test}
\end{figure}

\paragraph{Bandwidth Tests}
\subparagraph{osu\_bw - Bandwidth Test} The general behavior of this test is the expected and desired. It can be observed in figure \nameref{OSU Bandwidth Test} As the window size increases, the bandwidth required for transmission is increased, until the point at which the maximum bandwidth is reached, which is near 6000 MB/s. In general, the different versions of Open MPI outside the container do not represent a differentiating factor in terms of bandwidth. However, in the central values it is possible to observe a slight advantage under the use of Open MPI 1.10.2. Anyway, in the "critical" values, which could be considered those in which the bandwidth is being consumed in its entirety, they all have the same behavior.
\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{osu_bw}
      \caption{OSU Bandwidth Test}
       \label{OSU Bandwidth Test}
\end{figure}

\subparagraph{osu\_bibw - Bidirectional Bandwidth Test}
Now, problem complexity will be increased, making both the nodes involved send out a fixed number of back-to-back messages and wait for the reply. Due to the success of the uni-directional Bandwidth test, the expected result to this test is the double value achievement, compared to the uni-directional test. Contrary to what might be expected, the behavior shown is not stable. As can be seen in figure \nameref{OSU Bidirectional Bandwidth Test}, once the window size is bigger than 8192MB, results appear chaotic and without apparent sense. Version 1.10.2 appears to be the most stable, but its high scalability once the 8192MB threshold is passed can be interpreted as a bad signal too.
\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{osu_bibw}
      \caption{OSU Bidirectional Bandwidth Test}
       \label{OSU Bidirectional Bandwidth Test}
\end{figure}

\subparagraph{osu\_mbw\_mr - Multiple Bandwidth}
Now the number of cores involved in the problem will be increased. A total of four cores (belonging to four different nodes) were used. So, the waited behavior is to have two senders and two receivers, what will cause the obtaining of double the bandwidth, compared to the uni-directional test, since the result are added. Fortunately this is what it was obtained as result, as can be seen in figure \nameref{OSU Multiple Bandwidth Test}.
\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{osu_mbw_mr_bandwidth}
      \caption{OSU Multiple Bandwidth Test}
       \label{OSU Multiple Bandwidth Test}
\end{figure}
\subparagraph{osu\_mbw\_mr - Message Rate Test}
Analyzing the message rate, if we deal with small sized messages, such as those shown at the beginning of the graph \nameref{OSU Message Rate Test}, a big number of them can be sent, thanks to the big bandwidth available. As its size increases, the bandwidth will not be enough to send so many. Therefore, the behavior should be approximately the reverse of the \nameref{OSU Multiple Bandwidth Test} graph (understanding behavior as figure shape, not its values!). As an add-on, it can be said that for smaller sized messages as the outside Open MPI version is higher, its behavior is getting better.
\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{osu_mbw_mr_messages}
      \caption{OSU Message Rate Test}
       \label{OSU Message Rate Test}
\end{figure}

\hr
Now all the result test are available, it is clear that the InfiniBand network is been used, at least, in most cases. Parameters as the very low latency (6 microseconds or less) are critical to reach this conclusion. Furthermore, the obtained results are in agreement with those obtained by other teams using the same tests, like UL HPC MPI Tutorial: Building and Runnning OSU Micro-Benchmarks by the University of Luxembourg High Performance Computing (HPC) Team\cite{UL-HPC-MPI-tutorial}; Some Micro-benchmarks for HPC: 10Gb Ethernet on EC2 vs QDR Infiniband\cite{EC2-vs-QDR-Infiniband}, by Adam DeConinck, HPC Cluster Administrator at Los Alamos National Laboratory; or Implementation and comparison of RDMA over Ethernet\cite{implementation-comparison-rdma-over-ethernet}, belonging to National Security Education Center, Los Alamos National Laboratory. Reader is free to consult such documents if you want further information. \textbf{It seems that containers with different inside-outside Open MPI versions make use of the InfiniBand network, but due to some detected anomalies, the results are not completely conclusive}.

\subsection{RAM benchmarks: STREAM}
The \textit{STREAM} benchmark\cite{stream} is a simple synthetic benchmark program that measures sustainable memory bandwidth (in MB/s) and the corresponding computation rate for simple vector kernels. The reason why the \textit{STREAM} benchmark was the selected, and not another one, is because \textit{STREAM} is the de facto industry standard benchmark for measuring sustained memory bandwidth. It uses data sets much larger than the available cache on a system, which avoids large amounts of time devoted waiting for cache misses to be satisfied. Because of this reason, \textit{STREAM} documentation indicates that the value of the \verb|STREAM_ARRAY_SIZE| must be at least four times larger than the combined size of all last level caches used in the run. But this is not the only limitation to define the value of \verb|STREAM_ARRAY_SIZE|: Its size should be large enough so that the "timing calibration" output by the program is at least 20 clock-ticks. Example: most versions of Windows have a 10 millisecond timer granularity. 20 "ticks" at 10 ms/tic is 200 milliseconds. If the chip is capable of 10 GB/s, it moves 2 GB in 200 msec. This means the each array must be at least 1 GB, or 128M elements.


To run the Stream Benchmark on multiprocessor, there are several choices: OpenMP, pthreads, and MPI. Pursuing a global consistency, we will use the MPI version of \textit{STREAM}\footnote{Specifically, stream\_mpi.c, the version of STREAM MPI coded in C, without more reason than the knowledge of the language by the writter of this document.}, instead of its default version, which uses OpenMP. Thank of this version, we will be able to measure the employed RAM under a multiprocessor multinode container environment, using an InfiniBand network, that is, after all, the goal of this research. This will require MPI support installed, an already well-known requirement.


If the reader of this document wishes to go further into the operation of STREAM, online official documentation\cite{stream-documentation} can be consulted. \newline
Turning to our particular case, Finis Terrae II nodes have 24 cores, with the following memory cache structure:
\begin{itemize}
\item L1d cache: 32K
\item L1i cache: 32K
\item L2 cache: 256K
\item L3 cache: 30720K
\end{itemize}


So, attending to the previous limitations to define the value of the \verb|STREAM_ARRAY_SIZE|, it is important to consider the values of the last level cache, as well as the number of available cores. As a plus requirement, added because of the MPI use, two different nodes will be used, in order to ensure the MPI and InfiniBand network utilization. Besides, it is important to reserve the nodes by they full (employing the 24 cores per node and all the available RAM), obtaining the benchmark execution in a blocking way under this environment, without other parallel processes that could generate noise. Doing some arithmetic, we can observe that the sum total of the last level cache per node is 720MiB (30720K = 30MiB per core, having 24 cores). Then, a 720MiB four times size will be needed (2880MiB should be a correct array size), considering a simple-node execution. Since we will execute the benchmark using two nodes, we will have twice the cache: 1440MiB on its last level, obtaining now a size of 5760MB as a great value for the \verb|STREAM_ARRAY_SIZE|. So, I will define the value of the \verb|STREAM_ARRAY_SIZE| as 760000000, which should be a value larger enough for our purposes. 


The other limitation factor, the "timing calibration" output by the program, can not be considered until the program is executed, so it will be ignored at the moment. 


Continuing with the values under the benchmark will be done, it is important to consider that STREAM runs each kernel \verb|NTIMES| times and reports the best result for any iteration after the first. The selected value for this variable will be the default one: 10 times. 


Finally, keep in mind that we must compile the code with optimization and enabling the correct options to be able to use multiprocessing; employing OpenMPI mpicc in our case. Array size can be set at compile time without modifying the source code for the (many) compilers that support preprocessor definitions on the compile line.
\begin{lstlisting}[language=bash,caption={Compilation example employing Singularity containers}]
sudo singularity exec -w CONTAINER_NAME.img mpicc -m64 -o /usr/bin/stream_mpi -O -DSTREAM_ARRAY_SIZE=760000000 stream_mpi.c
\end{lstlisting}


Once the test were executed and the results were obtained, we can analyze them and obtain a series of conclusions. First of all, we will check the values that has to do with the array size and its implications.
\begin{itemize}
\item Total Aggregate Array size = 760000000 (elements)
\item Total Aggregate Memory per array = 5798.3 MiB (= 5.7 GiB).
\item Total Aggregate memory required = 17395.0 MiB (= 17.0 GiB).
\item Data is distributed across 48 MPI ranks
\begin{itemize}
\item   Array size per MPI rank = 15833333 (elements)
\item   Memory per array per MPI rank = 120.8 MiB (= 0.1 GiB).
\item   Total memory per MPI rank = 362.4 MiB (= 0.4 GiB).
\end{itemize}
\end{itemize}


As can be seen, the execution worked over a 760000000 elements array and over 48 different cores, employing MPI. This ensures the wished multinode multicore execution. Due to these parameters, some concrete size values are obtained; e.g. a total aggregate memory value of 17GiB. 


The second important execution output which must be reasoned is the timer granularity, inasmuch as it was previously defined as a limitation factor. The output value for the granularity/precision appears to be 1 microseconds, what will make each test to take on the order of 54269 microseconds (= 54269 timer ticks). Turning back our attention to the needed premises for a great execution, we can check that the array size should be large enough so that the "timing calibration" output by the program is at least 20 clock-ticks. As 54269 >> 20, we can conclude the array size is large enough, and the results will suppose a good study sample. 


The obtained results for the defined test are:
\begin{table}[H]
\centering
\caption{Container STREAM benchmark results}
\label{my-label}
\begin{tabular}{|c|c|l|c|c|c|}
\hline
Function & \multicolumn{2}{c|}{Best rate (MB/s)} & Avg time & Min time & Max time \\ \hline
Copy     & \multicolumn{2}{c|}{155550.7}         & 0.078218 & 0.078174 & 0.078288 \\ \hline
Scale    & \multicolumn{2}{c|}{154837.6}         & 0.078575 & 0.078534 & 0.078641 \\ \hline
Add      & \multicolumn{2}{c|}{177616.6}         & 0.102739 & 0.102693 & 0.102845 \\ \hline
Triad    & \multicolumn{2}{c|}{177573.8}         & 0.102803 & 0.102718 & 0.103148 \\ \hline
\end{tabular}
\end{table}


Continuing the memory benchmark study, a native execution will be done\footnote{In this case, the \texttt{stream\_mpi.c} program were compiled using the GNU compiler gcc/5.3.0, the 1.10.2 version of Open MPI, and enabling the optimization compiling flags.}, in order to compare both executions, the containerized one and the native one. Obviously, the parameters under the native execution will be done are exactly the same than those which were used in the previous containerized execution. That is why they will not be analyzed again. The obtained results under the native execution are:
\begin{table}[H]
\centering
\caption{Native STREAM benchmark results}
\label{my-label}
\begin{tabular}{|c|c|l|c|c|c|}
\hline
Function & \multicolumn{2}{c|}{Best rate (MB/s)} & Avg time & Min time & Max time \\ \hline
Copy     & \multicolumn{2}{c|}{155478.6}         & 0.078266 & 0.078210 & 0.078386 \\ \hline
Scale    & \multicolumn{2}{c|}{154717.4}         & 0.078623 & 0.078595 & 0.078691 \\ \hline
Add      & \multicolumn{2}{c|}{177729.3}         & 0.102662 & 0.102628 & 0.102683 \\ \hline
Triad    & \multicolumn{2}{c|}{177710.3}         & 0.102694 & 0.102639 & 0.102851 \\ \hline
\end{tabular}
\end{table}


Note that the test results have an avg error less than 1.000000e-13 on all three arrays for both cases, what make them a reliable source. 


Now that all the results were obtained, we can easily compare them. Starting with those related to the execution times, observe that they present values with differences that hover around the magnitude of 1.0e-4 seconds. In addition, there is no execution that always obtains the best results (less time), but in some cases this value is obtained by the native execution and in others by the containerized solution. Looking now at the bandwidth rate, there is no a clear difference between the results to assign a solution better than the other one. \newline 


So, after all the done tests and the obtained results, we can say that a containerized solution will make use of the machine memory that will come very close to the use that would be made under a native execution. \textbf{Native and container executions are comparable in terms of memory.}

\subsection{CPU benchmarks: HPL}
\textit{HPL}\cite{HPL}, a portable implementation of the \textit{High-Performance Linpack Benchmark} for distributed-memory computers, is a software package that solves a random dense linear system in double precision arithmetic on distributed-memory computers.


The \textit{HPL} package provides a testing and timing program to quantify the accuracy of the obtained solution as well as the time it took to compute it. It requires the availibility on the system of an implementation of the Message Passing Interface (MPI). An implementation of either the Basic Linear Algebra Subprograms BLAS or the Vector Signal Image Processing Library VSIPL is also needed. These requisites make \textit{HPL} an excelent CPU usage measurer candidate for our concrete research.


The end of this benchmark it is not to reach the infrastructure peak of performance, but what is pursued is to verify the functioning and to check
the correct performance mixing different MPI versions inside-outside containers to make a comparison with an one-to-one inside-outside MPI versions execution, already done.


What will be done is to reproduce a series of executions that were already made in the cluster, under an one-to-one inside-outside MPI versions. Thus, once all the results are obtained, both behaviors will be compared, and it will be reasoned if the solution with mixing MPI versions containers supposes, if it exists, a reasonable delay.


In order to get a battery of results a little varied, what will be done is to execute these benchmarks with an upward focus. By this I mean that I will reproduce its execution making use of a greater number of processors each time. Computations will be run in 1, 2, 4, 8, 16 and 32 nodes at FinisTerrae II. For every node increment, the problem's size will increase too, in order to take always approximately 80-90\% of available memory of the involved nodes. This is what is called a scalability test weak. \newline

The results of the one-to-one inside-outside MPI versions weak scaling tests show an increase of the aggregated performance as we increase the number of nodes involved. Looking at the results we can also see that the performance per node is maintained almost immutable along the different executions. These results are also very close to the expected theoretical values.

Figure \nameref{Weak scaling test 1} shows the performance (logarithmic scale) of the weak scaling test depending on the number of nodes involved in the computation.

To provide another view of the obtained values, they can also be seen in figure \nameref{Weak scaling test 2}, with a different scale for the vertical axis (GFlops).

\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{CPU-graph-1}
      \caption{Weak scaling test 1}
       \label{Weak scaling test 1}
\end{figure}

\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{CPU-graph-2}
      \caption{Weak scaling test 2}
       \label{Weak scaling test 2}
\end{figure}

Comparing those results with the inside-outside mixed MPI execution, it is clear that both have the same trend. However, in the mix test, a little delay is present. At this point, it is important to clarify that the test has been performed only once (a single execution), due to the large computational time that is required for this. Therefore, in the absence of comparisons with other executions, it is possible that this particular case has produced factors that generate noise, thus causing this delay. So, if we look at figures \nameref{Weak scaling test 3} and \nameref{Weak scaling test 4}, we will be able to appreciate this little delay, especially on the 32 nodes execution.

\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{CPU-graph-3}
      \caption{Weak scaling test 3}
       \label{Weak scaling test 3}
\end{figure}

\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{CPU-graph-4}
      \caption{Weak scaling test 4}
       \label{Weak scaling test 4}
\end{figure}

Therefore, despite the fact that the execution with inside-outside mixed MPI has presented a small delay, this is not of a great magnitude. In addition, this delay may occur only as a consequence of some uncontrolled noise. Therefore, we can say that the \textbf{execution with inside-outside mixed MPI is a good solution in terms of computational time, although it may not be the best.}

\section{Discussion 1/2-1 page}
Ut eget est sem. Duis suscipit turpis sed orci mattis, sed tempus sapien rhoncus. Aliquam at ex nulla. Suspendisse et lorem ornare, tincidunt eros et, dictum metus. Etiam auctor elementum enim. Fusce in convallis ex, at sagittis elit. Suspendisse auctor gravida molestie. In mi tellus, tristique quis mi sed, blandit lobortis libero. 


\begin{thebibliography}{9}
\bibitem{easybuild}
  Easybuild [Internet] - EasyBuild: building software with ease. [Quoted August 03, 2017]. 
Recovered from: \url{<https://easybuilders.github.io/easybuild/>}

\bibitem{dockerhub}
  Docker Hub [Internet] [Quoted August 04, 2017]. 
Recovered from: \url{<https://hub.docker.com/>}

\bibitem{singularityhub}
  Singularity Hub [Internet] [Quoted August 04, 2017]. 
Recovered from: \url{<https://singularityhub.com/>}

\bibitem{osu-microbenchmarks}
  OSU Micro Benchmarks [Internet] - MVAPICH: MPI over InfiniBand, Omni-Path, Ethernet/iWARP, and RoCE [Quoted July 24, 2017]. 
Recovered from: \url{<http://mvapich.cse.ohio-state.edu/userguide/virt/#_osu_micro_benchmarks>}

\bibitem{osu-microbenchmarks-readme}
  OSU Micro Benchmarks README file [Internet] - MVAPICH: MPI over InfiniBand, Omni-Path, Ethernet/iWARP, and RoCE [Quoted July 24, 2017].
Recovered from: \url{<http://mvapich.cse.ohio-state.edu/static/media/mvapich/README-OMB.txt>}

\bibitem{UL-HPC-MPI-tutorial}
  HPC workflow with MPI Parallel/Distributed jobs (OSU Microbenchmarks, HPL) [Internet] - University of Luxembourg High Performance Computing (HPC) Team [Quoted August 03, 2017]. 
Recovered from: \url{<https://ulhpc-tutorials.readthedocs.io/en/latest/advanced/OSU_MicroBenchmarks/>}

\bibitem{EC2-vs-QDR-Infiniband}
  Some Micro-benchmarks for HPC: 10Gb Ethernet on EC2 vs QDR Infiniband [Internet] - Thinking out loud, personal Adam DeConinck blog [Quoted August 03, 2017]. 
Recovered from: \url{<https://blog.ajdecon.org/some-micro0-benchmarks-for-hpc-10gb-ethernet-o/>}

\bibitem{implementation-comparison-rdma-over-ethernet}
  Implementation and comparison of RDMA over Ethernet [Internet] - National Security Education Center - Los Alamos National Laboratory [Quoted August 03, 2017]. 
Recovered from: \url{<http://www.lanl.gov/projects/national-security-education-center/information-science-technology/_assets/docs/2010-si-docs/Team_CYAN_Implementation_and_Comparison_of_RDMA_Over_Ethernet_Presentation.pdf>}

\bibitem{stream}
  STREAM: Sustainable Memory Bandwidth in High Performance Computers [Internet] [Quoted July 28, 2017]. 
Recovered from: \url{<https://www.cs.virginia.edu/stream/>}

\bibitem{stream-documentation}
  STREAM ref. - STREAM: Sustainable Memory Bandwidth in High Performance Computers [Internet] [Quoted July 28, 2017]. 
Recovered from: \url{<https://www.cs.virginia.edu/stream/ref.html>}

\bibitem{HPL}
  HPL - A Portable Implementation of the High-Performance Linpack Benchmark for Distributed-Memory Computers [Internet] - The Innovative Computing Laboratory (ICL) of the University of Tennesse [Quoted July 28, 2017]. 
Recovered from: \url{<http://www.netlib.org/benchmark/hpl/>}

\end{thebibliography}
\end{document}
