\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{nameref}
\usepackage{pifont}
\usepackage{hyperref}
\usepackage{indentfirst}
\usepackage{lipsum}
\usepackage{listings}
\lstset{basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{red},
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
  keywordstyle=\color{blue}
}
\usepackage{fourier}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand\hr{\par\vspace{-.5\ht\strutbox}\noindent\hrulefill\par}

\title{MPI interversion compability with Singularity containers over Finis Terrae II using an InfiniBand network}

\author{Manuel Simon Novoa}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Enter a short summary here. What topic do you want to investigate and why? What experiment did you perform? What were your main results and conclusion?
\end{abstract}

\section{Introduction 5 lines to max 1/2 page}
\label{sec:introduction}
\newpage

\section{Architecture description}
\label{sec:architecture}
The tests and executions named in the following document were launched it all under the hardware described in this section. As this research belong to the \textit{Supercomputing Center of Galicia (CESGA)}, the selected machine to perform the development was the \textit{Finis Terrae II}.

\textit{Finis Terrae} is a scientific computing infrastructure managed by \textit{CESGA} and integrated in RES (the Spanish Supercomputing Network), a Spanish Unique Scientific and Technical Installation (ICTS). It is a linux based heterogeneous cluster, with an Infiniband FDR low latency network interconnecting 317 computing nodes based on Intel Xeon Hasswell processors. Each node is composed by 24 cores and has a 128GB main memory per server. It is
connected to a shared Lustre High-performance Filesystem with 768TB of disk
space. Additionally, the system has 4 GPU servers with
GPUs (NVIDIA K80) and 2 servers with Intel Xeon Phi accelerators. There is also
one "Fat" node with 8 Intel Haswell 8867v3 processors, 128 cores and 4TB of
main memory. Together, these nodes are able to provide a computing power of 328 TFLOPS, 44,8 TB of RAM and 1,5 PB of disk capacity. The system includes a high performance parallel storage system able to achieve a speed of more than 20 GB/s.

\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{infraFTII}
      \caption{Finis Terrae II Infrastructure}
       \label{Finis Terrae II Infrastructure}
\end{figure}

\section{Research motivation}
\label{sec:motivation}
Traditional software deployment performed in HPCs present some issues which make it very inflexible. Different approaches, like could be the containerization technology, can provide an interesting alternative that is more flexible with very little impact on performance.

Traditional software deployment and integration consists in performing all the needed steps, like download, configure, build, test and install, to have the software project natively in a production infrastructure. The main goal of this task is to provide the software with good performance and ready to use for end users.

Scientific software is extremely complex from an architectural point of view. It is usually composed of numerous mathematical concepts and features implemented along several software components in order to provide high level abstraction layers. These layers can be implemented in the software project itself or integrated via third party libraries or software components. The whole environment of each scientific software is usually composed of a complex dependency matrix and, at least, one programming language and compiler.

Isolation and integration of all dependency matrices at HPC clusters are traditionally managed by environment modules. Environment Modules provide a way to dynamically change the user environment through module files. Lmod, a Lua based module system, is being used at FinisTerrae II.

The key advantage of environment modules is that it allows to use multiple versions of a program or package from the same account by just loading the proper module file. In general, module files are created on per application per version basis. They can be dynamically loaded, unloaded, or switched. Along with the capability of using multiple versions of the same software it also can be used to implement site policies regarding the access and use of applications.

Module files allow managing the loading and unloading of environments to run a particular application, but to manage complex work-flows with environment modules can be sometimes unaffordable, and requires the re-installation of some tools with compatible dependencies. These issues are difficult to manage from the user and the administrator point of view.

Finally, the hardware and software ecosystem of an HPC production infrastructure is different than a development ecosystem, and usually a lot of unexpected issues appear while integrating and deploying the complex environment of mathematical frameworks. To integrate the whole environment of each software project in a production infrastructure is usually hard and time consuming.

Also,as this software is evolving very fast and using the latest technologies and features of the compilers, new versions are provided very frequently. This puts a lot of pressure on the software support team of the infrastructures. Even though there are some tools that could help in this process, like \textit{EasyBuild}\cite{easybuild}, an application to manage software on High Performance Computing (HPC) systems in an efficient way.

So, what is intended to do is to find a simple and efficient way that allow to deploy and integrate that complex software with its dependencies. The goal is to reduce at maximum at possible the previously named time consuming. Most importantly, because end users can come up with endless combinations of software and dependencies, it is to achieve maximum portability, but without losing the computational capacity of the system.

\section{Technology selection}
\label{sec:technology-selection}
In accordance with the requirements mentioned in the previous section, a study of the state of the art was made to select the solution that best fits the presented problem.

It is very important to always keep in mind these premises, since they are the ones that will limit the selection of the solution:
\begin{itemize}
\item The operating system of the cluster can not be modified, since there are other jobs that are currently running in the same without any problem, and it is not possible to hinder its execution.
\item We must make use of the available hardware (shown in section \nameref{sec:architecture}) as efficiently as possible; that is, we can not limit the resources (CPU, memory, network, etc.)
\end{itemize}

Thus, some common virtualization solutions are:

\subsection{Emulation}
An emulator typically functions as an interpreter who reads the instructions from the virtual operating system (guest) and translates them into instructions that can run smoothly on the host operating system. In this way the emulator provides the virtual operating system with a simulated execution environment. The emulator itself runs like any other application on the host operating system. The emulated hardware may or may not match the actual machine on which the emulator is running. Emulators recreate a complete environment, including all CPU instructions and I/O devices using software, which is why performance is deeply affected.

Therefore, because of that problem with performance, the second premise can not be accomplished. Emulation solution does not seem to be the best.

\subsection{Virtualization of the Operating System - Containerization} 
This type of virtualization runs only a kernel, the host, which also attends calls from virtual systems. Virtualization software (usually called a container) uses a set of libraries that translate virtual operating system calls into the appropriate format to be executed on the hardware using the host operating system. The environment provided by the container may or may not match the actual machine on which it is run. In case it does not match, the translator will be quite complex. Each container runs in its own environment, although they all share the same resources. It is possible to reallocate these resources to implement quality of service. In addition, operating system virtualization has the advantage that overhead is minimal.

Therefore, this is a solution that allows flexibility in addition to a minimum overhead. 

Although the containerization techniques is a buzzword nowadays especially in the Datacenter and Cloud industry, the idea is quite old. Container or "chroot" (change root) was a Linux technology to isolate single processes from each other without the need of emulating different hardware for them. Containers are lightweight operating systems within the Host Operating system that runs them. It uses native instructions on the core CPU, without the requirement of any VMM (Virtual Machine Manager). The only limitation is that we have to use the Host operating systems kernel to use the existing hardware components, unlike with virtualization, where we could use different operating systems without any restriction at the cost of the performance overhead. This is the key feature for the project. We could use different software, libraries even different Linux distribution without reinstalling the system. This makes HPC systems more flexible and easy to use for scientists and developers.

Container technology has become very popular as it makes application deployment very easy and efficient. As people move from virtualization to container technology, many enterprises have adopted software container for cloud application deployment. There is a stiff competition to push different technologies in the market. Although Docker is the most popular, to choose the right technology, depending on the purpose, it is important to understand what each of them stands for and does.

Several containerization technologies (like LXC, Docker, Udocker and Singularity) have been tested in the context of this project, but finally, Docker was rejected because of its kernel requirements and security. For example FinisTerrae-II does not have the kernel required by Docker. Udocker and Singularity were developed specifically to be used in HPC environments, as we will describe below. Both of them are Docker-compatible, and help to empower end-users of HPC systems providing a contained location where to manage their installations and custom software. They are also a great solution for developers, one of the biggest benefits for them is to deliver their software in a controlled environment and ready-to-use.

In one hand, Udocker is a basic user tool to execute simple Docker containers in user space without requiring root privileges, which enables basic download and sequential execution of docker containers by non-privileged users in Linux systems. It can be used to access and execute the content of docker containers in Linux batch systems and interactive clusters that are managed by other entities such as grid infrastructures or externally managed batch or interactive systems.

Although the Udocker development team is working to integrate it with message passing interface libraries (MPI), unfortunately, it is not yet supported.

On the other hand, Singularity was designed focusing on HPC and allows to leverage the resources of whatever host in which the software is running. This includes HPC interconnects, resource managers, file systems, GPUs and/or accelerators, etc.

Singularity was also designed around the notion of extreme mobility of computing and reproducible science. Singularity is also used to perform HPC in the cloud on AWS, Google Cloud, Azure and other cloud providers. This makes it possible to develop a research work-flow on a laboratory or a laboratory server, then bundle it to run on a departmental cluster, on a leadership class supercomputer, or in the cloud.

The simple usage of Singularity allows users to manage the creation of new containers and also to run parallel applications easily. Figure \nameref{Singularity work-flow at Finis Terrae II} shows the work-flow for creating and running containers using Singularity at Finis Terrae II.

\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{singularity-workflow}
      \caption{Singularity work-flow at Finis Terrae II}
       \label{Singularity work-flow at Finis Terrae II}
\end{figure}

As we can see in the previous figure, users can create or pull images from public registries (like Docker Hub\cite{dockerhub} or Singularity Hub\cite{singularityhub}), and also import images from tar pipes. Once the image is created, singularity allows to execute the container in interactive mode, and test or run any contained application using batch systems. All the work-flow can be managed by a normal user at Finis Terrae II, except the bootstrap process that needs to be called by a superuser. We can use a virtual machine with superuser privileges to modify or adapt and image to the infrastructure using the bootstrap Singularity command.

This work-flow allow an automatized integration and deployment, deriving in a very flexible and portable solution.

\section{Involved technologies}
\label{sec:technologies}
The hardware under all the tests and executions were executed were already described in section \nameref{sec:architecture}. Now it is turn to describe all the involved software needed to perform the solution, as well as all the implications they could have.

\subsection{Host Operating System and kernel}
The Finis Terrae II host machine have a \textbf{\textit{Red Hat Enterprise Linux Server (RHEL) release 6.7 (Santiago)}}, with a kernel version \verb|Linux 2.6.32-573.12.1.el6.x86_64|. Fortunately, this kernel version is compatible with Singularity.

\subsection{Singularity}
In order to understand Singularity's operation mode, some explanations will be done: 

\paragraph{Process Flow}
\subparagraph{}
The goal of Singularity is to run an application within a contained environment such as it was not contained. Thus there is a balance between what to separate and what not to separate. At present the virtualized namespaces are process, mount points, and certain parts of the contained file system.

On the other hand, Singularity does not support user escalation or context changes, nor does it have a root owned daemon process managing the container namespaces. It also exec’s the process work-flow inside the container and seamlessly redirects all IO in and out of the container directly between the environments. This makes doing things like MPI, X11 forwarding, and other kinds of work tasks trivial for Singularity.\newline

When executing container commands, the Singularity process flow can be generalized as follows:
\begin{enumerate}
\item Singularity application is invoked
\item Global options are parsed and activated
\item The Singularity command (subcommand) process is activated
\item Subcommand options are parsed
\item The appropriate sanity checks are made
\item Environment variables are set
\item The Singularity Execution binary is called (sexec)
\item Sexec determines if it is running privileged and calls the SUID code if necessary
\item Namespaces are created depending on configuration and process requirements
\item The Singularity image is checked, parsed, and mounted in the CLONE\_NEWNS namespace
\item Bind mount points are setup
\item The namespace CLONE\_FS is used to virtualize a new root file system Singularity calls \verb|execvp()| and Singularity process itself is replaced by the process inside the container
\item When the process inside the container exists, all namespaces collapse with that process, leaving a clean system
\end{enumerate}
All of the above steps take approximately 15-25 thousandths of a second to run, which is fast enough to seem instantaneous. \newline

\paragraph{Images}
\subparagraph{} 
Singularity images are single files which physically contains the container. The effect of all files existing virtually within a single image greatly simplifies sharing, copying, branching, and other management tasks.

You do not need admin/sudo to use Singularity containers. You do however need admin/root access to install Singularity and to build/manage your containers and images, but to use the containers you do not need any additional privileges to run programs within it.

Because Singularity is based on container principals, when an application is run from within a Singularity container its default view of the file system is different from how it is on the host system. This is what allows the environment to be portable. This means that root (/) inside the container is different from the host!

Singularity automatically tries to resolve directory mounts such that things will just work and be portable with whatever environment you are running on. This means that /tmp and /var/tmp are automatically shared into the container as is /home. Additionally, if you are in a current directory that is not a system directory, Singularity will also try to bind that to your container.

Note that there is a caveat in that a directory must already exist within your container to serve as a mount point. If that directory does not exist, Singularity will not create it for you! You must do that.

\paragraph{Resources sharing}
 \subparagraph{}
Singularity does no network isolation because it is designed to run like any other application on the
system. It has all of the same networking privileges as any program running as that user.

Singularity also allows you to leverage the resources of whatever host you are on. This includes HPC interconnects, resource managers, file systems, GPUs and/or accelerators, etc. Singularity does this by enabling several key facets:
\begin{itemize}
\item Encapsulation of the environment
\item Containers are image based
\item No user contextual changes or root escalation allowed
\item No root owned daemon processes
\end{itemize}

\paragraph{Glibc}
\subparagraph{}
There is also some level of glibc forward compatibility that must be taken into consideration for any container system. For example, I can take a Centos-5 container and run it on Centos-7, but I can not take a Centos-7 container and run it on Centos-5. So, If you require kernel dependent features, a container platform is probably not the right solution for you.

\paragraph{Bootstrap}
\subparagraph{}
Bootstrapping is the process where we install an operating system and then configure it appropriately for a specified need. To do this we use a bootstrap definition file which is a recipe of how to specifically build the container.

The bootstrap command is useful for creating a new bootstrap or modifying an existing one using a definition file that describes how to build the container.\newline

A Bootstrap definition file contains two main parts:
\begin{enumerate}
\item \textbf{Header}: The core operating system to bootstrap whithin the container.

The Bootstrap: keyword Identifies the singularity module (yum, debootstrap, arch, docker) that will be used for building the core components of the OS. Depending on the loaded module several keywords can be used to particularize the installation (MirrorURL, OSVersion, Include, From, etc.)
\item \textbf{Sections}: Shell scriptlets to run during the bootstrap process. The Boostrap sections are:
\begin{itemize}
\item setup: A Bourne shell scriptlet which will be executed on the host outside the container during bootstrap. 
\item post: A Bourne shell scriptlet executed during bootstraping from inside the container.
\item runscript: A persistent scriptlet in the container that will be executed via the singularity run command.
\item test: A persistent scriplet in the container that will be executed via the singularity test command. This section will be also executed at the end of the bootstrap process.
\end{itemize}
\end{enumerate}

\hr
Since Singularity operation should already be clear, only the Singularity versions used can be indicated: In the \textbf{local machine}, where root permissions are available, the Singularity version employed is \textbf{2.3-dist}. The Singularity version available on the \textbf{cluster} is \textbf{Singularity 2.2.1}.

\subsection{MPI}
\paragraph{Libraries}
\subparagraph{}
Since it is intended to use MPI, libraries will be used to enable this task. The selection of these libraries was done just checking those already installed on the cluster. The main efforts were put in the properly work of Open MPI, but some tests were done with Intel MPI too. This is the list with the MPI libraries and versions which are available on the cluster:
\begin{itemize}
\item Open MPI
  \begin{itemize}
  \item Open MPI 1.10.2
  \item Open MPI 2.0.0
  \item Open MPI 2.0.1
  \item Open MPI 2.0.2
  \item Open MPI 2.1.1
\end{itemize}
\item Intel MPI
  \begin{itemize}
  \item Intel MPI 5.1
  \item Intel MPI 2017
  \end{itemize}
\end{itemize}

\paragraph{MPI Singularity integration}
\subparagraph{}
As Singularity is the final selected technology selection to manage containers at Finis Terrae II, its relation with High Performance Computing will be enlarged. The most critical point is the correct use of MPI. \newline

Singularity developers ensure that their product has the ability to properly integrate with the Message Passing Interface (MPI)\cite{mpi-singularity}: \textit{"Work has already been done for out of the box compatibility with Open MPI (both in Open MPI v2.1.x as well as part of Singularity)."} Later in this document this will be tested. \newline

The Open MPI/Singularity workflow works as follows:

\begin{enumerate}
\item mpirun is called by the resource manager or the user directly from a shell
\item     Open MPI then calls the process management daemon (ORTED)
\item     The ORTED process launches the Singularity container requested by the mpirun command
\item     Singularity builds the container and namespace environment
\item     Singularity then launches the MPI application within the container
\item     The MPI application launches and loads the Open MPI libraries
\item     The Open MPI libraries connect back to the ORTED process via the Process Management Interface (PMI)
\item     At this point the processes within the container run as they would normally directly on the host.
\end{enumerate}

This entire process happens behind the scenes, and from the user's perspective running via MPI is as simple as just calling mpirun on the host as they would normally. \newline

To achieve proper containerized Open MPI support, Open MPI version 2.1 should be used. However, Singularity team explain that there are  three caveats:

\begin{enumerate}
\item     Open MPI 1.10.x may work but we expect you will need exactly matching version of PMI and Open MPI on both host and container (the 2.1 series should relax this requirement).
\item     Open MPI 2.1.0 has a bug affecting compilation of libraries for some interfaces (particularly Mellanox interfaces using libmxm are known to fail). If your in this situation you should use the master branch of Open MPI rather than the release.
\item     Using Open MPI 2.1 does not magically allow your container to connect to networking fabric libraries in the host. If your cluster has, for example, an infiniband network you still need to install OFED libraries into the container. Alternatively you could bind mount both Open MPI and networking libraries into the container, but this could run afoul of glib compatibility issues (its generally OK if the container glibc is more recent than the host, but not the other way around).
\end{enumerate}

Singularity utilizes a hybrid MPI container approach, this means that MPI must exist both inside and outside the container. \newline

There are some important considerations to work with MPI:
\begin{itemize}
\item OpenMPI must be newer of equal to the version inside the container.
\item To support InfiniBand, the container must support it.
\item To support PMI, the container must support it.
\item Very little (if any) performance penalty has been observed.
  \begin{itemize}
  \item MPI/Singularity invocation pathway\footnote{OpenMPI compatibility}:
    \begin{enumerate}
    \item From shell (or resource manager) mpirun gets called
    \item mpirun forks and exec orte daemon
    \item Orted process creates PMI
    \item Orted forks == to the number of process per node requested
    \item Orted children exec to original command passed to mpirun (Singularity)
    \item Each Singularity execs the command passed inside the given container
    \item Each MPI program links in the dynamic Open MPI libraries (ldd)
    \item Open MPI libraries continue to open the non-ldd shared libraries (dlopen)
    \item Open MPI libraries connect back to original orted via PMI
    \item All non-shared memory communication occurs through the PMI and then to local interfaces (e.g. InfiniBand)
    \end{enumerate}
  \end{itemize}
\end{itemize}


\subsection{Container Operating System and distribution}
At first, what was intended to do was to obtain different Linux distributions in each container, with different Open MPI versions in their repositories. The following commands were employed to know the available Open MPI version for each container:

\begin{lstlisting}[language=bash,caption={Open MPI versions check commands}]
apt-cache policy openmpi-common
apt-get install devscripts && rmadison openmpi-common
\end{lstlisting}

The difference between \verb|apt-cache| and \verb|rmadison| is that \verb|apt-cache| shows only the information known to the system (but can be used offline) while \verb|rmadison| shows all versions of available packages. Thank to that, it is possible to check if some Open MPI is already installed on a container by default or not, and the Open MPI versions available on its repositories. This allow a simple installation with aptitude, whenever the package manager is APT.
For example, for a Kali-based container:
\begin{lstlisting}[language=bash,caption={Open MPI versions available for Kali}]
rmadison openmpi-common 
openmpi-common | 1.4.5-1    | oldoldstable       | all
openmpi-common | 1.6.5-9.1  | oldstable-kfreebsd | all
openmpi-common | 2.0.2-2    | stable             | all
openmpi-common | 2.1.1-6    | testing            | all
openmpi-common | 2.1.1-6    | unstable           | all
\end{lstlisting}

In this example, Open MPI versions 2.0.2 and 2.1.1 are available, so we can install them easily (e.g. \verb|apt-get install openmpi-common/2.1.1|). Multiple distributions were proved, like Debian, Ubuntu, Kali or Fedora. However, because the availability of the different versions of Open MPI in the official repositories was not as varied as expected, the approach was changed. \newline

What was done in the final approach was to prepare different containers with exactly the same Operating System and installed on each one a different version of Open MPI, but this time, downloaded from the Open MPI official page. The selected distribution was Ubuntu 16.04.2 LTS (Ubuntu Xenial) because it is the current distribution provided by the Docker Hub if a Ubuntu container is requested (without indicating the version).
The way Open MPI is downloaded and installed was automatized in a bash script ...

\subsection{Available compilers}
Adding a new variable to the problem, the employed compiler must be taken into account. The cluster has a good battery of different compilers, that is:
\begin{itemize}
\item GNU compilers
  \begin{itemize}
  \item gcc/5.3.0
  \item gcc/6.1.0
  \item gcc/6.3.0
  \end{itemize}
\item Intel compilers
  \begin{itemize}
  \item intel/2016
  \item intel/2017
  \end{itemize}
\end{itemize}

Changing to the compiler employed into the containers, it was that one which is included per default with the selected distribution, that is it: \verb|5.4.0-6ubuntu1~16.04.4| (gcc/5.4.0).

\subsection{PMI}
TODO.

\hr

Then, what it is expected is some conflicts with some of these named variables and technologies. E.g. an application works properly under GNU compilers but not under Intel ones. This will be analyze in the following sections.

\section{Environment study}
Now all variables have been presented, we will present the environment study in a more specific way.

Six containers were created in total. Five of them have Open MPI installed, while the last one was configured with Intel MPI inside. All them make use of \textit{Ubuntu 16.04.2 LTS (Ubuntu Xenial)}, downloaded from the Docker Hub. The creation and configuration of the containers with Open MPI was completely automatized, making use of a bash script and the bootstrap definition files. By contrast, the container with Intel MPI required a much more manual installation, because of the installation process required by this software.

Remember that Singularity does not magically allow containers to connect to networking fabric libraries in the host. As the cluster makes use of an InfiniBand network, it will be needed to install OFED libraries into the container. So, all the needed libraries will be installed during the creation process of the containers. That is, it is important to specify all this software downloading, installation and configuration into the bootstrap configuration file.

For a clearer and more concise language, from now on, whenever we refer to containers with Open MPI inside, we will do it with the following nomenclature: container ompi/Open MPI version. E.g. if we are talking about a \textit{Ubuntu 16.04.2 LTS (Ubuntu Xenial)} container with Open MPI version 1.10.2, we will just call it: container ompi/1.10.2. In case of Intel MPI, the nomenclature will be changed to container impi/Intel MPI version (e.g. container impi/2017).

That from the container side. Changing to the outside values, we already know that the cluster is a \textit{Red Hat Enterprise Linux Server release 6.7}, which have different Open MPI and Intel MPI versions installed. So, every time we want to talk about an external configuration that make use of a concrete version of Open MPI or Intel MPI we will call it external ompi/version or external impi/version.

Then, different approaches will be made in order to arrive at a solution that allows the use of different versions of MPI libraries inside and outside the container. We will focus out attention on the ompi configurations, as it is a more standardized configuration, but some tests with impi configurations will be done too.

Understanding a little more the number of test needed for every approach we can see the table below.

\begin{table}[h]
\centering
\caption{My caption}
\label{my-label}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container ompi/} & \multicolumn{5}{c|}{External ompi/} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 &  &  &  &  &  \\ \hline
2.0.0 &  &  &  &  &  \\ \hline
2.0.1 &  &  &  &  &  \\ \hline
2.0.2 &  &  &  &  &  \\ \hline
2.1.1 &  &  &  &  & \\ \hline
\end{tabular}
\end{table}

As we deal with five different versions of Open MPI, their mixtures will give rise to 25 tests. But in addition, we must execute the same under different compilers, since they can be a determinant variable, and its implication will not be clear until the tests are realized. Not all external ompi configurations have the same compilers available, but they almost always have at least two different compilers. Also, we must repeat these test at least twice, one execution in just one node, and the other one in multiple nodes. This means that each approach will take approximately 100 executions. It is important to automate the process as much as possible because it is a tedious and repetitive task.

Finally, it is important to specify that on the Finis Terrae II there are two different process managers. The first one is \textit{Slurm}\cite{slurm}, a resource manager, which include the \textit{srun} tool. \textit{Srun} is a tool employed to manage the resources and the processes. Besides, OpenMPI provide \textit{mpirun} as process manager, but it has not the same functionalities than \textit{srun}.

\section{Approaches of the problem and development}
\label{sec:approach}
\subsection{Tests}
The selected application for the ompi tests was \verb|"ring"|\cite{ring}· It is one of the most basics applications, what ensures the correct environment performance, as well as Open MPI installation is working properly.

It works as follow: The ring program initializes a value from process zero, and the value is passed around every single process. The program terminates when process zero receives the value from the last process. First process makes sure that it has completed its first send before it tries to receive the value from the last process. All of the other processes simply call \verb|MPI_Recv| (receiving from their neighboring lower process) and then \verb|MPI_Send| (sending the value to their neighboring higher process) to pass the value along the ring. \verb|MPI_Send| and \verb|MPI_Recv| will block until the message has been transmitted. Because of this, the \verb|printfs| should occur by the order in which the value is passed.  \newline

For the impi tests, Intel MPI Library\cite{impi} comes with a set of source files for simple MPI programs that enable to test the installation. Test program sources are available for all supported programming languages and are located at \verb|<installdir>/test|, where \verb|<installdir>| is \verb|/opt/intel/compilers_and_libraries_<version>.x.xxx/linux/mpi| by default. The installation directory for the impi containers was the default one, replacing version for 2017.4.196. The source files available into this directory are:

\begin{lstlisting}[language=bash,caption={Intel MPI source files for simple MPI programs}]
.
|-- test.c
|-- test.cpp
|-- test.f
|-- test.f90
|-- Test.java
\end{lstlisting}

Since the Open MPI test program (\verb|ring_c.c|) was written in C, the C version of the Intel test program was also chosen. To compile the selected program, we use the Intel MPI compiler, available on \verb|/opt/intel/compilers_and_libraries_2017.4.196/linux/bin/intel64/icc|. The program functioning is the simplest possible: a \verb|printf| saying Hello World in a for loop, which includes every functional unity.

\subsubsection{Mixing MPI versions}
The first approach was the more intuitive and expected. That is, try a simple application that make use of MPI, mixing the Open MPI and Intel MPI inside and outside the container. For this first approach, the selected process manager was \textit{mpirun}.

\paragraph{MPIRUN}
\subparagraph{}
The first test done was the mix of the different ompi possibilities, including the different available compilers. For these executions, the obtained results were a one-to-one version compatibility. That is, for the properly program execution, the version of Open MPI must match exactly inside and outside the container. However, the same conclusion was obtained for the different conjugations with the different compilers available. This means that, until proven otherwise, the compiler will not be in itself a limiting factor. In addition, for this particular environment, the results were the same for single node cases as for multiple node cases. Results can be visualized on table \nameref{test1}. 

\begin{table}[H]
\centering
\caption{MPIRUN mixing different compilers, 1 and 2 nodes. Open MPI tests.}
\label{test1}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container ompi/} & \multicolumn{5}{c|}{External ompi/} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.0 & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.1 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.2 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} \\ \hline
2.1.1 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} \\ \hline
\end{tabular}
\end{table}

The same configuration was employed to prove the impi possible configurations. In this case, the selected program was \verb|test.c| instead of \verb|ring_c.c|. Due to some complications to obtain a containerized impi/5.1 only a container impi/2017 could be tested. The obtained results were the same than witch the impi tests, only one-to-one matches worked properly. This can be observed on table \nameref{test2}

\begin{table}[H]
\centering
\caption{MPIRUN mixing different compilers, 1 and 2 nodes. Intel MPI tests.}
\label{test2}
\begin{tabular}{|r|c|c|}
\hline
\multicolumn{1}{|c|}{Container Intel MPI version} & \multicolumn{2}{c|}{FT2 Intel MPI version} \\ \hline
 & 5.1 & 2017 \\ \hline
5.1 &  &  \\ \hline
2017 & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} \\ \hline
\end{tabular}
\end{table}

\hr
Note: Due to the difficulties to prove the impi configurations, from this moment, only test with Open MPI will be done. 


\paragraph{SRUN}
\subparagraph{}
Continuing the first presented approach, now we will execute the exactly same tests, but under the \textit{srun} process manager.

As could be expected, results were not the same. The big difference is that under the \textit{srun} process manager, the multiple nodes executions were not successful. Only executions under one node and with one-to-one matches for the Open MPI versions inside and outside the container were successful. This behavior can be observed on table \nameref{test3}
\begin{table}[H]
\centering
\caption{SRUN mixing different compilers, 1 node. Open MPI tests.}
\label{test3}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container OpenMPI version} & \multicolumn{5}{c|}{FT2 OpenMPI version} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.0 & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.1 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.2 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} \\ \hline
2.1.1 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \color[HTML]{006600}{\cmark} \\ \hline
\end{tabular}
\end{table}

The results for multiple nodes can be observed on table \nameref{test4} When this test was done, false positives were obtained for every one-to-one match. In those cases, when the ring program was executed, it did an execution for every node, but none of them could "see" the others. Because of this reason, the ring call was executed as many instances as nodes were indicated, having each of them just one process per ring. So, parallel communication was not obtained.
\begin{table}[H]
\centering
\caption{SRUN mixing different compilers, 2 nodes. Open MPI tests.}
\label{test4}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container OpenMPI version} & \multicolumn{5}{c|}{FT2 OpenMPI version} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.0 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.1 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.0.2 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
2.1.1 & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} \\ \hline
\end{tabular}
\end{table}

\hr
Remember there were some important considerations to work with MPI:
\begin{itemize}
  \item Open MPI must be newer of equal to the version inside the container.
  \item To support InfiniBand, the container must support it.
  \item To support PMI, the container must support it.
\end{itemize}

We have made sure to meet all these requirements to obtain the correct operation of MPI, as indicated by the Singularity team. All containers support InfiniBand and PMI, thanks to the OFED libraries installed during the container creation process. Theoretically, under this environment, all tests made under ompi containers with a lower Open MPI version than the cluster Open MPI version employed in each moment, should work properly. However, attending to the results, we can ensure that after a series of practical tests, interversion MPI compatibility can not be obtained making simple mixes.

After this, the simple mixing approach was abandoned, and we turned into a new approach.

\subsubsection{Binding libraries and its dependencies}
\label{sec:linking}

Since the previous approach, which consists of simply mixing the different MPI versions did not work, we will give it a twist and we will apply a completely different approach. Singularity containers allow us to bind some files and folders as we want to from the host. So, we will try to bind all the Open MPI needed libraries and its dependencies. Then, what we are trying to do at all is to employ exactly the same version of Open MPI inside and outside the container, what we already know it works properly. Its advantage is that it will be absolutely transparent for the final user. When a container arrive, it will be executed into a environment which will bind its Open MPI libraries to make use of the cluster MPI libraries, instead of their own. In counterpart, the big and obvious disadvantage is that the container philosophy will be lost. Containers will pass from use their own resources to use those what belong to the host, what supposes a portability lost. With this I mean that if we want to use this approach on a different cluster, we must adjust the libraries binding for every unique case. However, as we execute this solution under the Finis Terrae II, it could suppose a solution that contributes with a high level of flexibility and portability. That is, the cluster should accept a big and varied quantity of containers which make use of MPI.

Browsing the Singularity web we can found suggestion of how the binding should be done\cite{binding}. Although this recommendations are made for GPU drivers, Open MPI binding is named too as an alternative configuration solution. The explication is not very exhaustive, but at least it is considered by the Singularity team. Several test must be done in order to achieve the final binding solution.

\paragraph{SRUN}
\subparagraph{ldd}
\subparagraph{Tests apps: ring, hellowo}

\begin{table}[H]
\centering
\caption{srun ring 2 nodes, mixing compilers}
\label{my-label}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container OpenMPI version} & \multicolumn{5}{c|}{FT2 OpenMPI version} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 & \color[HTML]{006600}{\cmark}  & \color{green}{\danger} & \color{green}{\danger} & \color{green}{\danger} & \color{green}{\danger} \\ \hline
2.0.0 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} \\ \hline
2.0.1 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} \\ \hline
2.0.2 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} \\ \hline
2.1.1 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} \\ \hline
\end{tabular}
\end{table}
As can be seen at the table above, once all the involved libraries and its dependencies were satisfied, the test program, ring, could be executed under any container-FT2 OpenMPI mix. However, if the 1.10.2 OpenMPI container is used with a superior OpenMPI version outside, it will show a warning prompt, making reference to  different sizes in symbols (e.g. Symbol 'ompi\_mpi\_comm\_world' has different size in shared object, consider re-linking). Due to this, future real world applications could not work properly or crash.

\subsection{Real world applications}
Once the test applications passed; real world applications, built inside already created containers, were proved. For this reason, the OpenMPI version contained could not be changed. As we achieved a solution what seems work properly, we will make use of it for these executions. That is, we will check if our solution is suitable for big and complex problems. As these containerized programs belong to real world solutions, their proper execution should suppose a quality assurement for our achieved approach. We will test programs written in the following programing languages: \textit{C++}, \textit{Fortran} and \textit{Python}. All them were executed under two nodes.

\subsubsection{C++ application: Feel++}
\begin{table}[H]
\centering
\caption{Feel++}
\label{Feel++}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container ompi/} & \multicolumn{5}{c|}{External ompi/} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} \\ \hline
\end{tabular}
\end{table}
As can be seen, in this test, a containerized 1.10.2 OpenMPI version worked over 1.10.2, 2.0.0, 2.0.1 and 2.0.2 outside OpenMPI versions. Nevertheless, 2.1.1 outside OpenMPI did not work. Most probably this is because the latest version employs an Intel compiler, and not a GNU compiler as the other ones. So, for the very first time, a compiler dependency was discover. In order to get reduce noise as much as possible, from this moment, they are not taken into account OpenMPI versions that have not been compiled with GNU compilers.

\subsubsection{Fortran application: XH5For}
\begin{table}[H]
\centering
\caption{XH5For}
\label{XH5For}
\begin{tabular}{|r|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Container ompi/} & \multicolumn{5}{c|}{External ompi/} \\ \hline
\multicolumn{1}{|c|}{} & 1.10.2 & 2.0.0 & 2.0.1 & 2.0.2 & 2.1.1 \\ \hline
1.10.2 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} \\ \hline
2.1.1 & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \color[HTML]{006600}{\cmark} & \textcolor{red}{\xmark} \\ \hline
\end{tabular}
\end{table}

\section{Benchmarks}


One of the biggest containers' advantages is its theoretical efficient and lightweight operating mode. When using the same kernel as the host machine, and avoiding replication as much as possible, makes of containers virtualization a great solution for HPC programs execution.


Nevertheless, until this lightness is verified in our work environment, we will not stop talking about a completely theoretical framework. Because of this, some benchmarks will be done, to verify this in a practical way.


So, the objective of the following benchmark tests is no other than proof that containers development will allow a great response time.
\subsection{InfiniBand network benchmarks: OSU microbenchmarks}
Since the goal of this document is the description of how to use Singularity containers employing different versions of MPI, one of the most interesting benchmarks that can be done is the use of the network. We are not talking about a network that connects with the outside, but an internode network that allows the communication of the different processors for a correct parallel operation. For this purpose, FinisTerrae II, as well as many others clusters, uses an InfiniBand network. InfiniBand is a computer-networking communications standard used in high-performance computing that features very high throughput and very low latency. It is used for data interconnect both among and within computers.


So, now the Singularity containers are using MPI, we will check if they are making use of the InfiniBand network for that. 


The selected bechmarks for this practical approach are the \textit{OSU microbenchmarks}\cite{osu-microbenchmarks}, belonging to \textit{The Ohio State University}, what will allow us to measure some interesting parameters, such as latency or bandwidth. Once the tests were done, we will check if the obtained results belong to a reasonable InfiniBand network parameters. The objective of this test is not to obtain the best possible results in relation to the network, but only to see if the containers are actually making use of the InfiniBand network, so no real optimizations were done with the network.


When the \textit{OSU microbenchmark} pack is downloaded in our container, it can be compilled and installed easily thanks to the included makefile.
\begin{lstlisting}[language=bash,caption={Installing OSU microbenchmarks}]
sudo singularity exec -w CONTAINER_NAME.img ./configure CC=/path/to/special/mpicc --prefix=<path-to-install> && make && make install
\end{lstlisting}


Once the \textit{OSU microbenchmarks} are installed, we can find a battery of test in the directory \verb|/usr/bin/libexec/osu-micro-benchmarks| (default installation directory). This battery of benchmarks is composed by the following:
\begin{lstlisting}[language=bash,caption={OSU microbenchmarks}]
.
`-- osu-micro-benchmarks
    `-- mpi
        |-- collective
        |   |-- osu_allgather
        |   |-- osu_allgatherv
        |   |-- osu_allreduce
        |   |-- osu_alltoall
        |   |-- osu_alltoallv
        |   |-- osu_barrier
        |   |-- osu_bcast
        |   |-- osu_gather
        |   |-- osu_gatherv
        |   |-- osu_iallgather
        |   |-- osu_iallgatherv
        |   |-- osu_ialltoall
        |   |-- osu_ialltoallv
        |   |-- osu_ialltoallw
        |   |-- osu_ibarrier
        |   |-- osu_ibcast
        |   |-- osu_igather
        |   |-- osu_igatherv
        |   |-- osu_iscatter
        |   |-- osu_iscatterv
        |   |-- osu_reduce
        |   |-- osu_reduce_scatter
        |   |-- osu_scatter
        |   `-- osu_scatterv
        |-- one-sided
        |   |-- osu_acc_latency
        |   |-- osu_cas_latency
        |   |-- osu_fop_latency
        |   |-- osu_get_acc_latency
        |   |-- osu_get_bw
        |   |-- osu_get_latency
        |   |-- osu_put_bibw
        |   |-- osu_put_bw
        |   `-- osu_put_latency
        |-- pt2pt
        |   |-- osu_bibw
        |   |-- osu_bw
        |   |-- osu_latency
        |   |-- osu_latency_mt
        |   |-- osu_mbw_mr
        |   `-- osu_multi_lat
        `-- startup
            |-- osu_hello
            `-- osu_init
\end{lstlisting}


The working of these different benchmarks can be consulted on the \verb|README| file which is downloaded along with the benchmarks. If we do not want to download the \textit{OSU microbenchmarks} package to read this file, it can be also consulted online\cite{osu-microbenchmarks-readme}.


As can be seen, a lot of different benchmarks are available on this package. We will focus our attention on the point-to-point (pt2pt) tests, what will allow us to measure and to evaluate the network's characteristics from the own containers.

\paragraph{Latency Tests}
\subparagraph{osu\_latency - Latency Test}
The latency tests are carried out in a ping-pong fashion. The sender sends a message with a certain data size to the receiver and waits for a reply from the receiver. The receiver receives the message from the sender and sends back a reply with the same data size. Many iterations of this ping-pong test are carried out and average one-way latency numbers are obtained. Blocking version of MPI functions (MPI\_Send and MPI\_Recv) are used in the tests.
\subparagraph{osu\_multi\_lat - Multi-pair Latency Test}
This test is very similar to the latency test. However, at the same instant multiple pairs are performing the same test simultaneously. In order to perform the test across just two nodes the hostnames must be specified in block fashion. 

\paragraph{Bandwidth Tests}
\subparagraph{osu\_bw - Bandwidth Test}
The bandwidth tests were carried out by having the sender sending out a fixed number (equal to the window size) of back-to-back messages to the receiver and then waiting for a reply from the receiver. The receiver sends the reply only after receiving all these messages. This process is repeated for several iterations and the bandwidth is calculated based on the elapsed time (from the time sender sends the first message until the time it receives the reply back from the receiver) and the number of bytes sent by the sender. The objective of this bandwidth test is to determine the maximum sustained date rate that can be achieved at the network level. Thus, non-blocking version of MPI functions (MPI\_Isend and MPI\_Irecv) were used in the test. 

\subparagraph{osu\_bibw - Bidirectional Bandwidth Test}
The bidirectional bandwidth test is similar to the bandwidth test, except that both the nodes involved send out a fixed number of back-to-back messages and wait for the reply. This test measures the maximum sustainable aggregate bandwidth by two nodes.

\subparagraph{osu\_mbw\_mr - Multiple Bandwidth / Message Rate Test}
The multi-pair bandwidth and message rate test evaluates the aggregate uni-directional bandwidth and message rate between multiple pairs of processes. Each of the sending processes sends a fixed number of messages (the window size) back-to-back to the paired receiving process before waiting for a reply from the receiver. This process is repeated for several iterations. The objective of this benchmark is to determine the achieved bandwidth and message rate from one node to another node with a configurable number of processes running on each node. 


\hr
Having understood the purpose and behavior of the tests to be performed, we proceed to explain how they will be executed. We will make use of a little battery of five containers with Ubuntu 16.04.2 LTS (Ubuntu Xenial). Each container will have installed inside different Open MPI versions: 1.10.2, 2.0.0. 2.0.1, 2.0.2 and 2.1.1. Then, the \textit{OSU microbenchmarks} will be executed under the Finis Terrae II employing these containers, and mixing them with different Open MPI versions on it (outside the containers). The selected outside Open MPI versions are: 1.10.2, 2.0.0 and 2.0.1. So, a total of fifteen executions per test will be done, in order to obtain more stable values.


To ensure the use of the InfiniBand network, the obtained values will be compared with some scientific papers that tell how they used the same tests to measure their own InfiniBand network. These texts belong to research centers or similar, so they will be considered completely reliable sources. Some of them compare the InfiniBand network values versus the Ethernet network values obtained under the same tests. So, if the results obtained under the Finis Terrae II hover around similar values, the InfiniBand network use will be assured.


Once the different benchmarks were executed, it is easily observable that the obtained results under the same Open MPI version out of the container are very stable for the five Open MPI version combinations within the container. However, this does not happen when we talk about total Open MPI combinations outside the containers (although in some cases it remains completely stable). Observed this behavior, what has been done is to perform the average of the values belonging to the same version of Open MPI outside the container, to perform a comparison between the three versions of Open MPI outside the container.

\paragraph{Latency Tests}
\subparagraph{osu\_latency - Latency Test} As can be shown in figure \nameref{OSU Latency Test}, all tests showed very stable results regardless of the version of Open MPI used. The latency values shown are very small, less than 6 microseconds with window sizes up to 8192 bytes.
\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{osu_latency}
      \caption{OSU Latency Test}
       \label{OSU Latency Test}
\end{figure}

\subparagraph{osu\_multi\_lat - Multi-pair Latency Test} For the execution of this test, four cores were employed, in four different nodes. Results can be observed in figure \nameref{OSU Multi-pair Latency Test}. It seems that by extending the complexity of the problem, making multiple pairs to perform the same test simultaneously, at the same instant, not suppose a problem for the latency. The obtained results show one more time an extraordinary stability and a very low latency.
\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{osu_multi_lat}
      \caption{OSU Multi-pair Latency Test}
       \label{OSU Multi-pair Latency Test}
\end{figure}

\paragraph{Bandwidth Tests}
\subparagraph{osu\_bw - Bandwidth Test} The general behavior of this test is the expected and desired. It can be observed in figure \nameref{OSU Bandwidth Test} As the window size increases, the bandwidth required for transmission is increased, until the point at which the maximum bandwidth is reached, which is near 6000 MB/s. In general, the different versions of Open MPI outside the container do not represent a differentiating factor in terms of bandwidth. However, in the central values it is possible to observe a slight advantage under the use of Open MPI 1.10.2. Anyway, in the "critical" values, which could be considered those in which the bandwidth is being consumed in its entirety, they all have the same behavior.
\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{osu_bw}
      \caption{OSU Bandwidth Test}
       \label{OSU Bandwidth Test}
\end{figure}

\subparagraph{osu\_bibw - Bidirectional Bandwidth Test}
Now, problem complexity will be increased, making both the nodes involved send out a fixed number of back-to-back messages and wait for the reply. Due to the success of the uni-directional Bandwidth test, the expected result to this test is the double value achievement, compared to the uni-directional test. Contrary to what might be expected, the behavior shown is not stable. As can be seen in figure \nameref{OSU Bidirectional Bandwidth Test}, once the window size is bigger than 8192MB, results appear chaotic and without apparent sense. Version 1.10.2 appears to be the most stable, but its high scalability once the 8192MB threshold is passed can be interpreted as a bad signal too.
\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{osu_bibw}
      \caption{OSU Bidirectional Bandwidth Test}
       \label{OSU Bidirectional Bandwidth Test}
\end{figure}

\subparagraph{osu\_mbw\_mr - Multiple Bandwidth}
Now the number of cores involved in the problem will be increased. A total of four cores (belonging to four different nodes) were used. So, the waited behavior is to have two senders and two receivers, what will cause the obtaining of double the bandwidth, compared to the uni-directional test, since the result are added. Fortunately this is what it was obtained as result, as can be seen in figure \nameref{OSU Multiple Bandwidth Test}.
\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{osu_mbw_mr_bandwidth}
      \caption{OSU Multiple Bandwidth Test}
       \label{OSU Multiple Bandwidth Test}
\end{figure}
\subparagraph{osu\_mbw\_mr - Message Rate Test}
Analyzing the message rate, if we deal with small sized messages, such as those shown at the beginning of the graph \nameref{OSU Message Rate Test}, a big number of them can be sent, thanks to the big bandwidth available. As its size increases, the bandwidth will not be enough to send so many. Therefore, the behavior should be approximately the reverse of the \nameref{OSU Multiple Bandwidth Test} graph (understanding behavior as figure shape, not its values!). As an add-on, it can be said that for smaller sized messages as the outside Open MPI version is higher, its behavior is getting better.
\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{osu_mbw_mr_messages}
      \caption{OSU Message Rate Test}
       \label{OSU Message Rate Test}
\end{figure}

\hr
Now all the result test are available, it is clear that the InfiniBand network is been used, at least, in most cases. Parameters as the very low latency (6 microseconds or less) are critical to reach this conclusion. Furthermore, the obtained results are in agreement with those obtained by other teams using the same tests, like UL HPC MPI Tutorial: Building and Runnning OSU Micro-Benchmarks by the University of Luxembourg High Performance Computing (HPC) Team\cite{UL-HPC-MPI-tutorial}; Some Micro-benchmarks for HPC: 10Gb Ethernet on EC2 vs QDR Infiniband\cite{EC2-vs-QDR-Infiniband}, by Adam DeConinck, HPC Cluster Administrator at Los Alamos National Laboratory; or Implementation and comparison of RDMA over Ethernet\cite{implementation-comparison-rdma-over-ethernet}, belonging to National Security Education Center, Los Alamos National Laboratory. Reader is free to consult such documents if you want further information. \textbf{It seems that containers with different inside-outside Open MPI versions make use of the InfiniBand network, but due to some detected anomalies, the results are not completely conclusive}.

\subsection{RAM benchmarks: STREAM}
The \textit{STREAM} benchmark\cite{stream} is a simple synthetic benchmark program that measures sustainable memory bandwidth (in MB/s) and the corresponding computation rate for simple vector kernels. The reason why the \textit{STREAM} benchmark was the selected, and not another one, is because \textit{STREAM} is the de facto industry standard benchmark for measuring sustained memory bandwidth. It uses data sets much larger than the available cache on a system, which avoids large amounts of time devoted waiting for cache misses to be satisfied. Because of this reason, \textit{STREAM} documentation indicates that the value of the \verb|STREAM_ARRAY_SIZE| must be at least four times larger than the combined size of all last level caches used in the run. But this is not the only limitation to define the value of \verb|STREAM_ARRAY_SIZE|: Its size should be large enough so that the "timing calibration" output by the program is at least 20 clock-ticks. Example: most versions of Windows have a 10 millisecond timer granularity. 20 "ticks" at 10 ms/tic is 200 milliseconds. If the chip is capable of 10 GB/s, it moves 2 GB in 200 msec. This means the each array must be at least 1 GB, or 128M elements.


To run the Stream Benchmark on multiprocessor, there are several choices: OpenMP, pthreads, and MPI. Pursuing a global consistency, we will use the MPI version of \textit{STREAM}\footnote{Specifically, stream\_mpi.c, the version of STREAM MPI coded in C, without more reason than the knowledge of the language by the writter of this document.}, instead of its default version, which uses OpenMP. Thank of this version, we will be able to measure the employed RAM under a multiprocessor multinode container environment, using an InfiniBand network, that is, after all, the goal of this research. This will require MPI support installed, an already well-known requirement.


If the reader of this document wishes to go further into the operation of STREAM, online official documentation\cite{stream-documentation} can be consulted. \newline
Turning to our particular case, Finis Terrae II nodes have 24 cores, with the following memory cache structure:
\begin{itemize}
\item L1d cache: 32K
\item L1i cache: 32K
\item L2 cache: 256K
\item L3 cache: 30720K
\end{itemize}


So, attending to the previous limitations to define the value of the \verb|STREAM_ARRAY_SIZE|, it is important to consider the values of the last level cache, as well as the number of available cores. As a plus requirement, added because of the MPI use, two different nodes will be used, in order to ensure the MPI and InfiniBand network utilization. Besides, it is important to reserve the nodes by they full (employing the 24 cores per node and all the available RAM), obtaining the benchmark execution in a blocking way under this environment, without other parallel processes that could generate noise. Doing some arithmetic, we can observe that the sum total of the last level cache per node is 720MiB (30720K = 30MiB per core, having 24 cores). Then, a 720MiB four times size will be needed (2880MiB should be a correct array size), considering a simple-node execution. Since we will execute the benchmark using two nodes, we will have twice the cache: 1440MiB on its last level, obtaining now a size of 5760MB as a great value for the \verb|STREAM_ARRAY_SIZE|. So, I will define the value of the \verb|STREAM_ARRAY_SIZE| as 760000000, which should be a value larger enough for our purposes. 


The other limitation factor, the "timing calibration" output by the program, can not be considered until the program is executed, so it will be ignored at the moment. 


Continuing with the values under the benchmark will be done, it is important to consider that STREAM runs each kernel \verb|NTIMES| times and reports the best result for any iteration after the first. The selected value for this variable will be the default one: 10 times. 


Finally, keep in mind that we must compile the code with optimization and enabling the correct options to be able to use multiprocessing; employing OpenMPI mpicc in our case. Array size can be set at compile time without modifying the source code for the (many) compilers that support preprocessor definitions on the compile line.
\begin{lstlisting}[language=bash,caption={Compilation example employing Singularity containers}]
sudo singularity exec -w CONTAINER_NAME.img mpicc -m64 -o /usr/bin/stream_mpi -O -DSTREAM_ARRAY_SIZE=760000000 stream_mpi.c
\end{lstlisting}


Once the test were executed and the results were obtained, we can analyze them and obtain a series of conclusions. First of all, we will check the values that has to do with the array size and its implications.
\begin{itemize}
\item Total Aggregate Array size = 760000000 (elements)
\item Total Aggregate Memory per array = 5798.3 MiB (= 5.7 GiB).
\item Total Aggregate memory required = 17395.0 MiB (= 17.0 GiB).
\item Data is distributed across 48 MPI ranks
\begin{itemize}
\item   Array size per MPI rank = 15833333 (elements)
\item   Memory per array per MPI rank = 120.8 MiB (= 0.1 GiB).
\item   Total memory per MPI rank = 362.4 MiB (= 0.4 GiB).
\end{itemize}
\end{itemize}


As can be seen, the execution worked over a 760000000 elements array and over 48 different cores, employing MPI. This ensures the wished multinode multicore execution. Due to these parameters, some concrete size values are obtained; e.g. a total aggregate memory value of 17GiB. 


The second important execution output which must be reasoned is the timer granularity, inasmuch as it was previously defined as a limitation factor. The output value for the granularity/precision appears to be 1 microseconds, what will make each test to take on the order of 54269 microseconds (= 54269 timer ticks). Turning back our attention to the needed premises for a great execution, we can check that the array size should be large enough so that the "timing calibration" output by the program is at least 20 clock-ticks. As 54269 >> 20, we can conclude the array size is large enough, and the results will suppose a good study sample. 


The obtained results for the defined test are:
\begin{table}[H]
\centering
\caption{Container STREAM benchmark results}
\label{my-label}
\begin{tabular}{|c|c|l|c|c|c|}
\hline
Function & \multicolumn{2}{c|}{Best rate (MB/s)} & Avg time & Min time & Max time \\ \hline
Copy     & \multicolumn{2}{c|}{155550.7}         & 0.078218 & 0.078174 & 0.078288 \\ \hline
Scale    & \multicolumn{2}{c|}{154837.6}         & 0.078575 & 0.078534 & 0.078641 \\ \hline
Add      & \multicolumn{2}{c|}{177616.6}         & 0.102739 & 0.102693 & 0.102845 \\ \hline
Triad    & \multicolumn{2}{c|}{177573.8}         & 0.102803 & 0.102718 & 0.103148 \\ \hline
\end{tabular}
\end{table}


Continuing the memory benchmark study, a native execution will be done\footnote{In this case, the \texttt{stream\_mpi.c} program were compiled using the GNU compiler gcc/5.3.0, the 1.10.2 version of Open MPI, and enabling the optimization compiling flags.}, in order to compare both executions, the containerized one and the native one. Obviously, the parameters under the native execution will be done are exactly the same than those which were used in the previous containerized execution. That is why they will not be analyzed again. The obtained results under the native execution are:
\begin{table}[H]
\centering
\caption{Native STREAM benchmark results}
\label{my-label}
\begin{tabular}{|c|c|l|c|c|c|}
\hline
Function & \multicolumn{2}{c|}{Best rate (MB/s)} & Avg time & Min time & Max time \\ \hline
Copy     & \multicolumn{2}{c|}{155478.6}         & 0.078266 & 0.078210 & 0.078386 \\ \hline
Scale    & \multicolumn{2}{c|}{154717.4}         & 0.078623 & 0.078595 & 0.078691 \\ \hline
Add      & \multicolumn{2}{c|}{177729.3}         & 0.102662 & 0.102628 & 0.102683 \\ \hline
Triad    & \multicolumn{2}{c|}{177710.3}         & 0.102694 & 0.102639 & 0.102851 \\ \hline
\end{tabular}
\end{table}


Note that the test results have an avg error less than 1.000000e-13 on all three arrays for both cases, what make them a reliable source. 


Now that all the results were obtained, we can easily compare them. Starting with those related to the execution times, observe that they present values with differences that hover around the magnitude of 1.0e-4 seconds. In addition, there is no execution that always obtains the best results (less time), but in some cases this value is obtained by the native execution and in others by the containerized solution. Looking now at the bandwidth rate, there is no a clear difference between the results to assign a solution better than the other one. \newline 


So, after all the done tests and the obtained results, we can say that a containerized solution will make use of the machine memory that will come very close to the use that would be made under a native execution. \textbf{Native and container executions are comparable in terms of memory.}

\subsection{CPU benchmarks: HPL}
\textit{HPL}\cite{HPL}, a portable implementation of the \textit{High-Performance Linpack Benchmark} for distributed-memory computers, is a software package that solves a random dense linear system in double precision arithmetic on distributed-memory computers.


The \textit{HPL} package provides a testing and timing program to quantify the accuracy of the obtained solution as well as the time it took to compute it. It requires the availibility on the system of an implementation of the Message Passing Interface (MPI). An implementation of either the Basic Linear Algebra Subprograms BLAS or the Vector Signal Image Processing Library VSIPL is also needed. These requisites make \textit{HPL} an excelent CPU usage measurer candidate for our concrete research.


The end of this benchmark it is not to reach the infrastructure peak of performance, but what is pursued is to verify the functioning and to check
the correct performance mixing different MPI versions inside-outside containers to make a comparison with an one-to-one inside-outside MPI versions execution, already done.


What will be done is to reproduce a series of executions that were already made in the cluster, under an one-to-one inside-outside MPI versions. Thus, once all the results are obtained, both behaviors will be compared, and it will be reasoned if the solution with mixing MPI versions containers supposes, if it exists, a reasonable delay.


In order to get a battery of results a little varied, what will be done is to execute these benchmarks with an upward focus. By this I mean that I will reproduce its execution making use of a greater number of processors each time. Computations will be run in 1, 2, 4, 8, 16 and 32 nodes at FinisTerrae II. For every node increment, the problem's size will increase too, in order to take always approximately 80-90\% of available memory of the involved nodes. This is what is called a scalability test weak. \newline

The results of the one-to-one inside-outside MPI versions weak scaling tests show an increase of the aggregated performance as we increase the number of nodes involved. Looking at the results we can also see that the performance per node is maintained almost immutable along the different executions. These results are also very close to the expected theoretical values.

Figure \nameref{Weak scaling test 1} shows the performance (logarithmic scale) of the weak scaling test depending on the number of nodes involved in the computation.

To provide another view of the obtained values, they can also be seen in figure \nameref{Weak scaling test 2}, with a different scale for the vertical axis (GFlops).

\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{CPU-graph-1}
      \caption{Weak scaling test 1}
       \label{Weak scaling test 1}
\end{figure}

\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{CPU-graph-2}
      \caption{Weak scaling test 2}
       \label{Weak scaling test 2}
\end{figure}

Comparing those results with the inside-outside mixed MPI execution, it is clear that both have the same trend. However, in the mix test, a little delay is present. At this point, it is important to clarify that the test has been performed only once (a single execution), due to the large computational time that is required for this. Therefore, in the absence of comparisons with other executions, it is possible that this particular case has produced factors that generate noise, thus causing this delay. So, if we look at figures \nameref{Weak scaling test 3} and \nameref{Weak scaling test 4}, we will be able to appreciate this little delay, especially on the 32 nodes execution.

\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{CPU-graph-3}
      \caption{Weak scaling test 3}
       \label{Weak scaling test 3}
\end{figure}

\begin{figure}[H]
\centering
     \includegraphics[width=1.0\textwidth]{CPU-graph-4}
      \caption{Weak scaling test 4}
       \label{Weak scaling test 4}
\end{figure}

Therefore, despite the fact that the execution with inside-outside mixed MPI has presented a small delay, this is not of a great magnitude. In addition, this delay may occur only as a consequence of some uncontrolled noise. Therefore, we can say that the \textbf{execution with inside-outside mixed MPI is a good solution in terms of computational time, although it may not be the best.}

\section{Discussion 1/2-1 page}
Ut eget est sem. Duis suscipit turpis sed orci mattis, sed tempus sapien rhoncus. Aliquam at ex nulla. Suspendisse et lorem ornare, tincidunt eros et, dictum metus. Etiam auctor elementum enim. Fusce in convallis ex, at sagittis elit. Suspendisse auctor gravida molestie. In mi tellus, tristique quis mi sed, blandit lobortis libero. 


\begin{thebibliography}{9}
\bibitem{easybuild}
  Easybuild [Internet] - EasyBuild: building software with ease. [Quoted August 03, 2017]. 
Recovered from: \url{<https://easybuilders.github.io/easybuild/>}

\bibitem{dockerhub}
  Docker Hub [Internet] [Quoted August 04, 2017]. 
Recovered from: \url{<https://hub.docker.com/>}

\bibitem{singularityhub}
  Singularity Hub [Internet] [Quoted August 04, 2017]. 
Recovered from: \url{<https://singularityhub.com/>}

\bibitem{mpi-singularity}
  Singularity on HPC [Internet] - Singularity [Quoted August 04, 2017]. 
Recovered from: \url{<http://singularity.lbl.gov/docs-hpc>}

\bibitem{ring}
  ring\_c.c code [Internet] - Open MPI (open-mpi) Github repository  [Quoted August 10, 2017]. 
Recovered from: \url{<https://raw.githubusercontent.com/open-mpi/ompi/master/examples/ring_c.c>}

\bibitem{impi}
  Intel® MPI Library [Internet] - Intel Developer Zone  [Quoted August 10, 2017]. 
Recovered from: \url{<https://software.intel.com/en-us/intel-mpi-library>}

\bibitem{slurm}
  Slurm Workload Manager Documentation [Internet] - Slurm Workload Manager [Quoted August 11, 2017]. 
Recovered from: \url{<https://slurm.schedmd.com/>}

\bibitem{binding}
  Using Host libraries: GPU drivers and OpenMPI BTLs [Internet] - Singularity [Quoted August 11, 2017]. 
Recovered from: \url{<http://singularity.lbl.gov/tutorial-gpu-drivers-open-mpi-mtls>}

\bibitem{osu-microbenchmarks}
  OSU Micro Benchmarks [Internet] - MVAPICH: MPI over InfiniBand, Omni-Path, Ethernet/iWARP, and RoCE [Quoted July 24, 2017]. 
Recovered from: \url{<http://mvapich.cse.ohio-state.edu/userguide/virt/#_osu_micro_benchmarks>}

\bibitem{osu-microbenchmarks-readme}
  OSU Micro Benchmarks README file [Internet] - MVAPICH: MPI over InfiniBand, Omni-Path, Ethernet/iWARP, and RoCE [Quoted July 24, 2017].
Recovered from: \url{<http://mvapich.cse.ohio-state.edu/static/media/mvapich/README-OMB.txt>}

\bibitem{UL-HPC-MPI-tutorial}
  HPC workflow with MPI Parallel/Distributed jobs (OSU Microbenchmarks, HPL) [Internet] - University of Luxembourg High Performance Computing (HPC) Team [Quoted August 03, 2017]. 
Recovered from: \url{<https://ulhpc-tutorials.readthedocs.io/en/latest/advanced/OSU_MicroBenchmarks/>}

\bibitem{EC2-vs-QDR-Infiniband}
  Some Micro-benchmarks for HPC: 10Gb Ethernet on EC2 vs QDR Infiniband [Internet] - Thinking out loud, personal Adam DeConinck blog [Quoted August 03, 2017]. 
Recovered from: \url{<https://blog.ajdecon.org/some-micro0-benchmarks-for-hpc-10gb-ethernet-o/>}

\bibitem{implementation-comparison-rdma-over-ethernet}
  Implementation and comparison of RDMA over Ethernet [Internet] - National Security Education Center - Los Alamos National Laboratory [Quoted August 03, 2017]. 
Recovered from: \url{<http://www.lanl.gov/projects/national-security-education-center/information-science-technology/_assets/docs/2010-si-docs/Team_CYAN_Implementation_and_Comparison_of_RDMA_Over_Ethernet_Presentation.pdf>}

\bibitem{stream}
  STREAM: Sustainable Memory Bandwidth in High Performance Computers [Internet] [Quoted July 28, 2017]. 
Recovered from: \url{<https://www.cs.virginia.edu/stream/>}

\bibitem{stream-documentation}
  STREAM ref. - STREAM: Sustainable Memory Bandwidth in High Performance Computers [Internet] [Quoted July 28, 2017]. 
Recovered from: \url{<https://www.cs.virginia.edu/stream/ref.html>}

\bibitem{HPL}
  HPL - A Portable Implementation of the High-Performance Linpack Benchmark for Distributed-Memory Computers [Internet] - The Innovative Computing Laboratory (ICL) of the University of Tennesse [Quoted July 28, 2017]. 
Recovered from: \url{<http://www.netlib.org/benchmark/hpl/>}

\end{thebibliography}
\end{document}
